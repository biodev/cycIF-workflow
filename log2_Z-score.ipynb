{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log2/Z-score Notebook\n",
    "\n",
    "By: Megan Grout (groutm2020@alumni.ohsu.edu)\n",
    "\n",
    "Adapted from code written by Dr. Marilyne Labrie and Nick Kendsersky\n",
    "\n",
    "\n",
    "Last updated: 20200527"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import external libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mplc\n",
    "import subprocess\n",
    "\n",
    "\n",
    "from scipy import signal\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \n",
    "import plotly.express as px\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import functions written for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycif_modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to change header names. Not encapsutated in `cycif_modules`, so that user can change on the fly as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may change for each experiment, so I have not sequestered\n",
    "# this code in the my_modules.py file\n",
    "\n",
    "# This function takes in a dataframe, changes the names\n",
    "# of the column in various ways, and returns the dataframe.\n",
    "# For best accuracy and generalizability, the code uses\n",
    "# regular expressions (regex) to find strings for replacement.\n",
    "def apply_header_changes(df):\n",
    "    # remove lowercase x at beginning of name\n",
    "    df.columns = df.columns.str.replace(\"^x\",\"\")\n",
    "    # remove space at beginning of name\n",
    "    df.columns = df.columns.str.replace(\"^ \",\"\")\n",
    "    # replace space with underscore\n",
    "    df.columns = df.columns.str.replace(\" \",\"_\")\n",
    "    # fix typos\n",
    "    #df.columns = df.columns.str.replace(\"typo\",\"correct_name\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for project\n",
    "base_dir = ''\n",
    "\n",
    "# Set name for of project\n",
    "# for use in directory creation\n",
    "project_name = 'ww'\n",
    "project_name = 'repro'\n",
    "project_name = 'gz_new'\n",
    "\n",
    "# Set string for current step, and for previous step\n",
    "# for use in file and direcotry naming\n",
    "step_suffix = 'zscore'\n",
    "previous_step_suffix_long = \"_bs\"\n",
    "\n",
    "# Initial input data directory\n",
    "input_data_dir = os.path.join(base_dir, project_name + previous_step_suffix_long)\n",
    "\n",
    "\n",
    "# log2/z-score directory\n",
    "output_data_dir = os.path.join(base_dir, project_name + \"_\" + step_suffix)\n",
    "\n",
    "# log2/z-score images subdirectory\n",
    "output_images_dir = os.path.join(output_data_dir,\"images\")\n",
    "\n",
    "# Metadata directories\n",
    "metadata_dir = os.path.join(base_dir, project_name + \"_metadata\")\n",
    "metadata_images_dir = os.path.join(metadata_dir,\"images\")\n",
    "\n",
    "# Create necessary directories for this step, if they don't already exist\n",
    "for d in [base_dir, input_data_dir, output_data_dir, output_images_dir, \n",
    "          metadata_dir, metadata_images_dir]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "# Change directory to location of input files        \n",
    "os.chdir(input_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of samples for use in this step of workflow. Do not include file extensions or steps labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comment for final workflow\n",
    "\n",
    "\n",
    "ls_samples = ['TMA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all metadata we need from the QC/EDA chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"marker_intensity_metadata.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "metadata = pd.read_csv(filename)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, metadata.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Verify headers\n",
    "exp_cols = ['Round','Target','Channel','target_lower','full_column','marker','location']\n",
    "compare_headers(exp_cols, metadata.columns.values, \"Marker metadata file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some of dataframe - FYI\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### not_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"not_intensities.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "not_intensities = []\n",
    "with open(filename, 'r') as fh:\n",
    "    not_intensities = fh.read().strip().split(\"\\n\")\n",
    "    # take str, strip whitespace, split on new line character\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, not_intensities.shape[0])\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Print to console\n",
    "print(\"not_intensities = \")\n",
    "print(not_intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full_to_short_column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"full_to_short_column_names.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "full_to_short_names = df.set_index('full_name').T.to_dict('records')[0]\n",
    "\n",
    "# Print information\n",
    "print('full_to_short_names =')\n",
    "print(full_to_short_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short_to_full_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"short_to_full_column_names.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "short_to_full_names = df.set_index('short_name').T.to_dict('records')[0]\n",
    "\n",
    "# Print information\n",
    "print('short_to_full_names =')\n",
    "print(short_to_full_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### color information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sample_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "df = df.drop(columns = ['hex'])\n",
    "\n",
    "# our tuple of float values for rgb, (r, g, b) was read in \n",
    "# as a string '(r, g, b)'. We need to extract the r-, g-, and b-\n",
    "# substrings and convert them back into floats\n",
    "df['rgb'] = df.apply(lambda row: rgb_tuple_from_str(row['rgb']), axis = 1)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "sample_color_dict = df.set_index('Sample_ID').T.to_dict('rgb')[0]\n",
    "\n",
    "# Print information\n",
    "print('sample_color_dict =')\n",
    "print(sample_color_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"channel_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "df = df.drop(columns = ['hex'])\n",
    "\n",
    "# our tuple of float values for rgb, (r, g, b) was read in \n",
    "# as a string '(r, g, b)'. We need to extract the r-, g-, and b-\n",
    "# substrings and convert them back into floats\n",
    "df['rgb'] = df.apply(lambda row: rgb_tuple_from_str(row['rgb']), axis = 1)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "channel_color_dict = df.set_index('Channel').T.to_dict('rgb')[0]\n",
    "\n",
    "# Print information\n",
    "print('channel_color_dict =')\n",
    "print(channel_color_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"round_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "df = df.drop(columns = ['hex'])\n",
    "\n",
    "# our tuple of float values for rgb, (r, g, b) was read in \n",
    "# as a string '(r, g, b)'. We need to extract the r-, g-, and b-\n",
    "# substrings and convert them back into floats\n",
    "df['rgb'] = df.apply(lambda row: rgb_tuple_from_str(row['rgb']), axis = 1)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "round_color_dict = df.set_index('Round').T.to_dict('rgb')[0]\n",
    "\n",
    "# Print information\n",
    "print('round_color_dict =')\n",
    "print(round_color_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"celltype_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "df = df.drop(columns = ['hex'])\n",
    "\n",
    "# our tuple of float values for rgb, (r, g, b) was read in \n",
    "# as a string '(r, g, b)'. We need to extract the r-, g-, and b-\n",
    "# substrings and convert them back into floats\n",
    "df['rgb'] = df.apply(lambda row: rgb_tuple_from_str(row['rgb']), axis = 1)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "celltype_color_dict = df.set_index('cell_type').T.to_dict('rgb')[0]\n",
    "\n",
    "# Print information\n",
    "print('celltype_color_dict =')\n",
    "print(celltype_color_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the first row of the file correpsonding to the first sample (index = 0)\n",
    "# in ls_samples\n",
    "\n",
    "# We do not need to specify a directory, since we earlier changed\n",
    "# the current working directory to be that containing these files\n",
    "filename = ls_samples[0] + previous_step_suffix_long + \".csv\"\n",
    "\n",
    "# Read in only the first line\n",
    "df = pd.read_csv(filename, index_col = 0, nrows = 1)\n",
    "\n",
    "# Apply the changes to the headers as specified in above funciton\n",
    "df = apply_header_changes(df)\n",
    "\n",
    "# Set variable to hold default header values\n",
    "expected_headers = df.columns.values\n",
    "\n",
    "print(\"df index name is currently\",df.index.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Used \" + ls_samples[0] + previous_step_suffix_long +\n",
    "      \".csv to determine the expected, corrected headers for all files.\")\n",
    "print(\"There headers are: \\n\" + \", \".join([h for h in expected_headers]) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dictionary to hold all individual sample data\n",
    "dfs = {}\n",
    "\n",
    "# iterate through each sample in our list of samples\n",
    "for sample in ls_samples:\n",
    "    # Check for existence of file\n",
    "    if not os.path.exists(sample+previous_step_suffix_long+\".csv\"):\n",
    "        print(\"File \" + sample+previous_step_suffix_long+\".csv\" +\n",
    "             \" does not exist. Removing from analysis...\")\n",
    "        # Remove from list if not found\n",
    "        ls_samples.remove(sample)\n",
    "        continue\n",
    "        \n",
    "    # open the file\n",
    "    # set the index to be the first (0-based indexing, so 0th)\n",
    "    # column in input file.\n",
    "    df = pd.read_csv(sample + previous_step_suffix_long + \".csv\", \n",
    "                     index_col = 0) #,  nrows = 500)\n",
    "    # use nrows to specify the number of rows you want\n",
    "    \n",
    "    # Check for empty df\n",
    "    # if so, don't continue trying to process df\n",
    "    if df.shape[0] == 0:\n",
    "        print('Zero content lines detected in ' + sample + ' file.'\n",
    "              'Removing from analysis...')\n",
    "        # Remove from list, so further steps won't be looking\n",
    "        # for data on this sample.\n",
    "        # Note that for lists, we do not need to re-assign\n",
    "        # the list when removing an item, i.e., we do not say\n",
    "        # 'ls_samples = ls_samples.remove(sample)', since this\n",
    "        # operation does not return anything.\n",
    "        ls_samples.remove(sample)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Verify that the loaded df are the right length\n",
    "    # commenting out because this code did not work on all\n",
    "    # machines during testing (failed one PC, succeeded with\n",
    "    # one PC and one MacBook)\n",
    "    try:\n",
    "        verify_line_no(sample + \".csv\", df.shape[0] + 1) \n",
    "    except:\n",
    "        pass\n",
    "    # adding 1 because we expect the header was detected \n",
    "    # during file import and not counted towards length of df\n",
    "    \n",
    "     # Manipulations necessary for concatenation\n",
    "    df = apply_header_changes(df)\n",
    "    # sort them alphanetically\n",
    "    df = df[[x for x in sorted(df.columns.values)]]\n",
    "    \n",
    "    # Compare headers of new df against what is expected\n",
    "    compare_headers(expected_headers, df.columns.values, sample)\n",
    "\n",
    "    # For cases where we have samples called TMA1.1, TMA1.2, TMA1.3, etc.\n",
    "    # Using regular expressions (regex) to extract the characters in the\n",
    "    # sample name from TMA to the following digits, stopping at the period\n",
    "    #if 'ROI_index' in df.columns.values:\n",
    "    #    df['ROI_slide'] = re.findall(r'(TMA\\d+)',sample)[0]    \n",
    "    \n",
    "    # Add to dictonary of dfs \n",
    "    dfs[sample] = df\n",
    "    \n",
    "\n",
    "#Merge dfs into one big df\n",
    "df = pd.concat(dfs.values(), ignore_index=False , sort = False)\n",
    "# remove dfs from memory, since its big (relatively) and we\n",
    "# don't need a data struture of all samples' data separated\n",
    "# individually when we can extract information from the big\n",
    "# df using the Sample_ID column\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few features to make sure our dataframe is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for NaN entries (should not be any unless columns do not align), which can result from stitching together dfs with different values in their headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are any null values, then print names of columns containing\n",
    "# null values\n",
    "if df.isnull().any().any():\n",
    "    print(df.columns[df.isnull().any()])\n",
    "\n",
    "#in 'if' statement, false means no NaN entries True means NaN entries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all expected files were imported into final dataframe by comparing our sample names to the unique values in the Sample_ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all expected files were imported into final dataframe\n",
    "\n",
    "if sorted(df.Sample_ID.unique()) == sorted(ls_samples):\n",
    "    print(\"All expected filenames present in big df Sample_ID column.\")\n",
    "else:\n",
    "    compare_headers(['no samples'], df.Sample_ID.unique(), \"big df Sample_ID column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log2 transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## no need to transpose df--non-intensity columns are present \n",
    "# in df but are not transformed by log2 nor by z-scoring\n",
    "\n",
    "# add 1 \n",
    "df.loc[:, ~df.columns.isin(not_intensities)] = \\\n",
    "    df.loc[:,~df.columns.isin(not_intensities)].copy() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply log2\n",
    "df.loc[:,~df.columns.isin(not_intensities)] = \\\n",
    "    np.log2(df.loc[:, ~df.columns.isin(not_intensities)])\n",
    "print('log2 transform finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-score transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score the rows (apply() with axis = 1, only perform on intensity data)\n",
    "df.loc[:,~df.columns.isin(not_intensities)] = \\\n",
    "    df.loc[:,~df.columns.isin(not_intensities)].apply(\n",
    "        lambda row: (row - row.median())/(row.std(ddof=0)), axis = 1)\n",
    "df.dropna(how = 'all', inplace = True, axis = 1)\n",
    "print('zscore rows finished')\n",
    "\n",
    "# Z-score the columns (apply() with axis = 0, only perform on intensity data)\n",
    "df.loc[:,~df.columns.isin(not_intensities)] = \\\n",
    "    df.loc[:,~df.columns.isin(not_intensities)].apply(\n",
    "        lambda row: (row - row.median())/(row.std(ddof=0)), axis = 0)\n",
    "df.dropna(how = 'all', inplace = True, axis = 1)\n",
    "print('zscore columns finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only be plotting ~10k cells in the interest of time/computing resources. We want these 10k lines in our original df to be sampled randomly, without replacement, with the caveat that the proportions of all samples in the data remains the same in this subset. If the size of the dataframe is > 10k rows, then we will proceed with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_row_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = create_subset(df, 'Sample_ID', subset_row_count, 'equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lines for each sample ID are in our subset df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the proportions of cells in the original and subset dfs compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sample_ID'].value_counts().sort_index()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()/subset_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data structures to map colors to columns and rows..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the row colors, we essentially just need to map the information in a given feature to the colors that correspond to that value in the right color dictionary. For example, it might be sample_3, sample_3, sample_4, , so we need the row colors to be (1, 1, 1), (1, 1, 1), (0, 0.25, 0.6). These are the initialy colors--if we are clustering rows or columns, the labels will still match the data with which they're associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row_colors = subset_df.Sample_ID.map(sample_color_dict)\n",
    "\n",
    "sample_row_colors[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_celltype_colors = subset_df.cell_type.map(celltype_color_dict)\n",
    "\n",
    "row_celltype_colors[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For column rows, matching up the information in each column with the appropriate color is more difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we want to translate marker columns to their corresponding channel information,\n",
    "# and then match that up with the right color, as with row columns\n",
    "\n",
    "# First, we merge the (L) non-intensity column values, transformed into a dataframe,\n",
    "# with the metadata df (R), matching on the \"0\" column present in the L,\n",
    "# which is the only column in there, with the \"full_column\" (aka df header name)\n",
    "# column in the R, only including all cases where there is a match and any unmatched\n",
    "# L cases ('both' [?] would be only cases where ther is is a match, and 'right' would\n",
    "# be cases with a match and any unmatched R columns).\n",
    "column_channel_colors = pd.merge(pd.DataFrame(pd.Series(\n",
    "    subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values)), \n",
    "                  metadata, how = 'left',\n",
    "         left_on = 0, right_on = 'full_column')[[0,'Channel']]['Channel'].map(channel_color_dict)\n",
    "\n",
    "# Set the index to be the names of the colors. There is only one column, and that is the corresponding\n",
    "# colors\n",
    "column_channel_colors.index = subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values\n",
    "\n",
    "column_channel_colors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we want to translate marker columns to their corresponding round information,\n",
    "# and then match that up with the right color, as with row columns\n",
    "\n",
    "# First, we merge the (L) non-intensity column values, transformed into a dataframe,\n",
    "# with the metadata df (R), matching on the \"0\" column present in the L,\n",
    "# which is the only column in there, with the \"full_column\" (aka df header name)\n",
    "# column in the R, only including all cases where there is a match and any unmatched\n",
    "# L cases ('both' [?] would be only cases where ther is is a match, and 'right' would\n",
    "# be cases with a match and any unmatched R columns).\n",
    "column_round_colors = pd.merge(pd.DataFrame(pd.Series(\n",
    "    subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values)), \n",
    "                  metadata, how = 'left',\n",
    "         left_on = 0, right_on = 'full_column')[[0,'Round']]['Round'].map(round_color_dict)\n",
    "\n",
    "# Set the index to be the names of the colors. There is only one column, and that is the corresponding\n",
    "# colors\n",
    "column_round_colors.index = subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values\n",
    "\n",
    "column_round_colors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data structure to hold everything we need for row/column annotations\n",
    "# annotations is a dictionary\n",
    "## IMPORTANT - if you use 'annotations', it MUST have both 'rows' and 'cols'\n",
    "## objects inside. These can be empty lists, but they must be there!\n",
    "anns = {}\n",
    "\n",
    "# create a data structure to hold everything we need for only row annotations\n",
    "# row_annotations is a list, where each item therein is a dictioary corresponding\n",
    "# to all of the data pertaining to that particular annotation\n",
    "# Adding each item (e.g., Sample, then Cluster), one at a time to ensure ordering\n",
    "# is as anticipated on figure\n",
    "row_annotations = []\n",
    "row_annotations.append({'label':'Sample','type':'row','mapping':sample_row_colors,'dict':sample_color_dict,\n",
    "                        'location':'center left','bbox_to_anchor':(0, 0.5)})\n",
    "#row_annotations.append({'label':'Cell type','type':'row','mapping':row_celltype_colors,\n",
    "#                        'dict':celltype_color_dict,\n",
    "#                        'location':'lower left','bbox_to_anchor':(0, 0.65)})# Add all row information into the annotations dictionary\n",
    "anns['rows'] = row_annotations\n",
    "\n",
    "\n",
    "# Now we repeat the process for column annotations\n",
    "col_annotations = []\n",
    "col_annotations.append({'label':'Round','type':'column','mapping':column_round_colors,'dict':round_color_dict,\n",
    "                       'location':'upper right','bbox_to_anchor':(1,0.50)})\n",
    "\n",
    "col_annotations.append({'label':'Column','type':'column','mapping':column_channel_colors,'dict':channel_color_dict,\n",
    "                       'location':'upper right','bbox_to_anchor':(1,0.75)})\n",
    "anns['cols'] = col_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually plot the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_function(\n",
    "    data = subset_df.loc[:,~subset_df.columns.isin(not_intensities)],\n",
    "    title = \"Z-score heatmap\",\n",
    "    # define method, metric, and color map\n",
    "    method = 'ward', metric = 'euclidean', cmap = 'coolwarm',\n",
    "    # colorbar (legend coloring of main plot) \n",
    "    cbar_kws = {'label':'Z-score Intens.'},\n",
    "    # xticklabels - want to have the nicknames instead of full names,\n",
    "    # so we translate from full to short names; we also only want to include\n",
    "    # non_intensity columns, to match the data we fed into under 'data'\n",
    "    xticklabels = [full_to_short_names[name] for name in \n",
    "                     subset_df.loc[:,\n",
    "                                 ~subset_df.columns.isin(not_intensities)].columns.values],\n",
    "    # where to save the dataframe \n",
    "    save_loc = output_images_dir,\n",
    "    # Boolean values for clustering\n",
    "    row_cluster = True, col_cluster = True,\n",
    "    # provide annotations established aboved\n",
    "    annotations = anns\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar plot of log2/z-score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sorted list of sample IDs\n",
    "samples = sorted(list(df.Sample_ID.copy().unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts for each Sample_ID, sorted by Sample_ID\n",
    "counts = pd.DataFrame(df.Sample_ID.value_counts()).sort_index()\n",
    "counts = counts.rename(columns = {'Sample_ID':'counts'})\n",
    "counts['Sample_ID'] = counts.index\n",
    "counts['color'] = counts.apply(lambda row: sample_color_dict[row['Sample_ID']], axis = 1)\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plot - cell counts by Sample_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "title = 'Cell counts by Sample_ID - log2 and z-score'\n",
    "\n",
    "for sample in ls_samples:\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=counts.loc[counts['Sample_ID']==sample,'Sample_ID'], \n",
    "        y = counts.loc[counts['Sample_ID']==sample,'counts'],\n",
    "        text = counts.loc[counts['Sample_ID']==sample,'counts'], textposition='outside',\n",
    "        marker=dict(\n",
    "            color='rgb' + str(sample_color_dict[sample])\n",
    "        ), showlegend = False\n",
    "    ))\n",
    "fig.update_layout(title = title, plot_bgcolor = 'white')\n",
    "fig.update_xaxes(title_text = 'Sample ID', linecolor = 'black')\n",
    "fig.update_yaxes(title_text = 'Cell count', linecolor = 'black')\n",
    "#plot(fig)\n",
    "fig.write_image(output_images_dir + \"/\" + title.replace(\" \",\"_\") + \".png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions of log2/z-score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-sample, per-marker distribution plots\n",
    "\n",
    "# change column names for visualization\n",
    "df = df.rename(columns = full_to_short_names)\n",
    "\n",
    "for sample in ls_samples:\n",
    "    for marker in [m for m in df.columns.values if m not in not_intensities]:\n",
    "        make_distr_plot_per_sample(\n",
    "        title = sample + \" \" + marker,# + \" Z-score\",\n",
    "        dfs = [\n",
    "            df.loc[df['Sample_ID']==sample,:].copy()],\n",
    "        df_names = [sample],\n",
    "        colors = [sample_color_dict[sample]],\n",
    "        x_label = \"Z-score intensity\",\n",
    "        legend = False,\n",
    "        markers = marker,\n",
    "        location = output_images_dir)    \n",
    "\n",
    "# reinstate column names\n",
    "df = df.rename(columns = short_to_full_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Pearson correlations and P values for all marker values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get we need to determine how many columns we will be evaulating. And prepare empty Numpy arrays to hold our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_corr_cols = len(df.columns[~df.columns.isin(not_intensities)])\n",
    "print(n_corr_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalues = np.empty((n_corr_cols, n_corr_cols))\n",
    "corrvalues = np.empty((n_corr_cols,n_corr_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a dataframe of only the columns we will use for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_corr = df.loc[:,~df.columns.isin(not_intensities)].copy()\n",
    "for_corr = for_corr.rename(columns = full_to_short_names)\n",
    "for_corr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we iterate through each pair of columns, calculate the Pearson correlation and associated p-value, and then store the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(for_corr.shape[1]):\n",
    "    for j in range(0,for_corr.shape[1]):\n",
    "        col1 = for_corr[for_corr.columns.values[i]]\n",
    "        col2 = for_corr[for_corr.columns.values[j]]\n",
    "        corrvalues[i,j] = pearsonr(col1,col2)[0]\n",
    "        pvalues[i,j] = pearsonr(col1,col2)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for visualization, we are rounding the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrvalues = pd.DataFrame(corrvalues).round(3)\n",
    "corrvalues.columns = for_corr.columns.values\n",
    "corrvalues.index = for_corr.columns.values\n",
    "\n",
    "pvalues = pd.DataFrame(pvalues)\n",
    "pvalues.columns = for_corr.columns.values\n",
    "pvalues.index = for_corr.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: no correlation value on plot, just put p value and have star is p<=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to append astetisk to row values meeting significance level\n",
    "def p_add_star(row):\n",
    "    sig_leve = 0.05\n",
    "    # This is a lon glist comprehension that uses if/else to determine output\n",
    "    m = [str('{:0.3e}'.format(m)) + \"*\" \n",
    "         if m <= 0.05 \\\n",
    "         else str('{:0.3e}'.format(m))\n",
    "        for m in row ]\n",
    "    return pd.Series(m)\n",
    "\n",
    "# Create a copy of the original df\n",
    "p_w_star = pvalues.copy()\n",
    "# Append asterisk to significant values\n",
    "p_w_star = p_w_star.apply(lambda row: p_add_star(row), axis = 1)\n",
    "# Supply column names - they were erased during the previous step\n",
    "p_w_star.columns = for_corr.columns.values\n",
    "\n",
    "# FYI - display\n",
    "p_w_star.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correlation visualization\n",
    "\n",
    "sb.set()\n",
    "\n",
    "# Label axes\n",
    "x_axis_labels = for_corr.columns.values.tolist()\n",
    "y_axis_labels = for_corr.columns.values.tolist()\n",
    "\n",
    "# Plot data\n",
    "ax = sb.heatmap(\n",
    "    # Data for plotting\n",
    "    corrvalues, \n",
    "    # Annotation - the display text on boxes\n",
    "    annot=p_w_star, \n",
    "    # Annotation keywords. \"size\" adjusts font sixe\n",
    "    annot_kws={\"size\": 1.25},\n",
    "    # Format = string for annotations\n",
    "    fmt='s',\n",
    "    # Set labels fo x and y axes\n",
    "    xticklabels=x_axis_labels, yticklabels=y_axis_labels,\n",
    "    # Color bay heywords. Here we label it to represent what the color represents\n",
    "    cbar_kws = {'label':'Pearson correlation'},\n",
    "    # Add black lines of designated width and color between boxes\n",
    "    linecolor = 'black', linewidth = 0.5,\n",
    "    # Colormap to use to color boxes\n",
    "    cmap = 'coolwarm'\n",
    "                )\n",
    "\n",
    "# Make axis tick lengths 0\n",
    "ax.tick_params(length=0)\n",
    "\n",
    "# Adjust y-axis labels\n",
    "plt.yticks(rotation=0, size = 5)\n",
    "\n",
    "# Adjust x-axis labels\n",
    "ax.xaxis.tick_top() # supply x-ticks along top of blot\n",
    "ax.xaxis.set_label_position('top') # move x-axis labels to top of plot, instead of along bottom\n",
    "plt.xticks(rotation=45, size = 5)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), ha='left') # align along left\n",
    "\n",
    "# Adjust title and general appearance\n",
    "ax.set_title(label = \"Correlations option 1\", fontsize = 20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot output\n",
    "filename = \"correlations_option1.png\"\n",
    "filename = os.path.join(output_images_dir, filename)\n",
    "plt.savefig(filename,dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: include correlation and 1-3 stars depending on p-value. 1 star: p-value <= 0.05; 2 stars: p-value <= 0.01; 3 stars: p-value <= 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to replace significance levels in rows with apprpriate number of stars based on threshold levels\n",
    "def p_to_star(row):\n",
    "    output  = []\n",
    "    thrd1 = 0.001\n",
    "    thrd2 = 0.01\n",
    "    thrd3 = 0.05\n",
    "    for item in row:\n",
    "        # Determine appropirate number of stars\n",
    "        if item <= thrd1:\n",
    "            stars = 3\n",
    "        elif item <= thrd2:\n",
    "            stars = 2\n",
    "        elif item <= thrd3:\n",
    "            stars = 1\n",
    "        else:\n",
    "            stars = 0\n",
    "        value = ''\n",
    "        # Construct star string of appr. length\n",
    "        for i in range(stars):\n",
    "            value += '*'\n",
    "        output.append(value)\n",
    "    return pd.Series(output)\n",
    "\n",
    "# Create copy of original df\n",
    "p_as_stars = pvalues.copy()\n",
    "# Replace values with appropriate number of stars\n",
    "p_as_stars = p_as_stars.apply(lambda row: p_to_star(row), axis = 1)\n",
    "# Suuply headers - previous step erased them\n",
    "p_as_stars.columns = for_corr.columns.values\n",
    "# FYI - display\n",
    "p_as_stars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View final df - join corr and stars\n",
    "corr_w_star = corrvalues.round(2).astype(str) + p_as_stars\n",
    "corr_w_star.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrvalues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_w_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Correlation visualization\n",
    "\n",
    "sb.set()\n",
    "\n",
    "# Axes labels\n",
    "x_axis_labels = for_corr.columns.values.tolist()\n",
    "y_axis_labels = for_corr.columns.values.tolist()\n",
    "\n",
    "# Construct plot\n",
    "ax = sb.heatmap(\n",
    "    # data to be plotted\n",
    "    corrvalues, \n",
    "    # Suuply annotations - the text to be displayed in each box\n",
    "    annot=corr_w_star, \n",
    "    # Annotation keywords - adjust font size\n",
    "    annot_kws={\"size\": 1.25},\n",
    "    # Format of annotation is string\n",
    "    fmt='s',\n",
    "    # Supply axies labels\n",
    "    xticklabels=x_axis_labels, yticklabels=y_axis_labels,\n",
    "    # Colorbar keywords - label for what the color scale represents\n",
    "    cbar_kws = {'label':'Pearson correlation'},\n",
    "    # Add lines of designated width and color between boxes\n",
    "    linecolor = 'black', linewidth = 0.5,\n",
    "    # Determine colormap to be used for coloring\n",
    "    cmap = 'coolwarm'\n",
    "                )\n",
    "\n",
    "# Make axis tick lengths 0\n",
    "ax.tick_params(length=0)\n",
    "\n",
    "# Adjust y-axis aesthetics\n",
    "plt.yticks(rotation=0, size = 5)\n",
    "\n",
    "# Adjust x-axis aesthetics\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top') # x-axis labels on top\n",
    "plt.xticks(rotation=45, size = 5)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), ha='left') # align left\n",
    "\n",
    "# Adjust title and general appearance\n",
    "ax.set_title(label = \"Correlations option 2\", fontsize = 20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot output\n",
    "filename = \"correlations_option2.png\"\n",
    "filename = os.path.join(output_images_dir, filename)\n",
    "plt.savefig(filename,dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save correlations and p-value data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"zscore_pearson_correlations.csv\"\n",
    "filename = os.path.join(output_data_dir, filename)\n",
    "corrvalues.to_csv(filename, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"zscore_pearson_p-values.csv\"\n",
    "filename = os.path.join(output_data_dir, filename)\n",
    "pvalues.to_csv(filename, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any other rows or columns we want to before saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of example, I will operate on a copy of df, called df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST do df.copy()\n",
    "# 'df_copy = df' would essentially \n",
    "# give you two different names for the\n",
    "# SAME dataframe, so operating on one\n",
    "# would also operate on the other\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operate on entire rows or columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "my_cols = []\n",
    "df_copy = df_copy.drop(columns = my_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only specific columns (explained below)\n",
    "my_cols = []\n",
    "my_cols = df.columns.values\n",
    "df_copy = df_copy.loc[:,my_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operate on rows and columns using filtering criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only certain rows based off of criteria\n",
    "\n",
    "# use df.loc[] to filter\n",
    "# df.loc[rows,columns]\n",
    "# df.loc[:,certain_cols] --> keep all rows ':', only certain cols\n",
    "# df.loc[certain_rows,:] --> keep only certain row, all cols ':'\n",
    "\n",
    "# Say we only want certain values for Sample_ID\n",
    "print(df_copy.Sample_ID.unique())\n",
    "keep = ['TMA1.1','TMA1.2','TMA1.3','TMA2.1','TMA2.2','TMA2.3']\n",
    "df_copy = df_copy.loc[df_copy['Sample_ID'].isin(keep),:]\n",
    "print(df_copy.Sample_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on multiple criteria\n",
    "# '&' or 'and'\n",
    "# '|' or 'or'\n",
    "# you MUST have parentheses around each logic expression!\n",
    "df_copy = df_copy.loc[\n",
    "    (df_copy['Sample_ID'].isin(['TMA1.1','TMA1.2','TMA1.3'])) \\\n",
    "    ## backslash above used to break line for readability, but tell Python to act like it's all one line\n",
    "        | (df_copy['Sample_ID'].isin(['TMA2.1','TMA2.2','TMA2.3'])),:]\n",
    "print(df_copy.Sample_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows based off of certain criteria\n",
    "# note the negating tilde '~'!\n",
    "\n",
    "df_copy = df_copy.loc[\n",
    "    (~df_copy['Sample_ID'].isin(['TMA1.1','TMA1.2','TMA1.3'])) \\\n",
    "    ## backslash above used to break line for readability, but tell Python to act like it's all one line\n",
    "        & (~df_copy['Sample_ID'].isin(['TMA2.1','TMA2.2','TMA2.3'])),:]\n",
    "print(df_copy.Sample_ID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data by Sample_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of output file first\n",
    "for sample in ls_samples:\n",
    "    filename = sample + \"_\" + step_suffix + \".csv\n",
    "    filename = os.path.join(output_data_dir, filename)\n",
    "    if os.path.exists(filename):\n",
    "        print(\"File by name \"+filename+\" already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output files\n",
    "for sample in ls_samples:\n",
    "    df_save = df.loc[df['Sample_ID'] == sample,:]\n",
    "    filename = sample + \"_\" + step_suffix + \".csv\"\n",
    "    filename = os.path.join(output_data_dir, filename)\n",
    "    df_save.to_csv(filename, index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

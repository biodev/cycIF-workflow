{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background subtraction notebook\n",
    "\n",
    "By: Megan Grout (groutm@ohsu.edu)\n",
    "\n",
    "Adapted from code written by Dr. Marilyne Labrie and Nick Kendsersky\n",
    "\n",
    "\n",
    "Last updated: 20191219"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import external libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mplc\n",
    "import subprocess\n",
    "\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \n",
    "import plotly.express as px\n",
    "init_notebook_mode(connected = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import function written for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycif_modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to change header names. Not encapsutated in `cycif_modules`, so that user can change on the fly as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may change for each experiment, so I have not sequestered\n",
    "# this code in the cycif_modules.py file\n",
    "\n",
    "# This function takes in a dataframe, changes the names\n",
    "# of the column in various ways, and returns the dataframe.\n",
    "# For best accuracy and generalizability, the code uses\n",
    "# regular expressions (regex) to find strings for replacement.\n",
    "def apply_header_changes(df):\n",
    "    # remove lowercase x at beginning of name\n",
    "    df.columns = df.columns.str.replace(\"^x\",\"\")\n",
    "    # remove space at beginning of name\n",
    "    df.columns = df.columns.str.replace(\"^ \",\"\")\n",
    "    # replace space with underscore\n",
    "    df.columns = df.columns.str.replace(\" \",\"_\")\n",
    "    # fix typos\n",
    "    df.columns = df.columns.str.replace(\"CKD1\",\"CDK1\")\n",
    "    df.columns = df.columns.str.replace(\"GAG3\",\"GATA3\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for project\n",
    "base_dir = '/Users/groutm/Desktop/weewin'\n",
    "base_dir = '/Users/groutm/Desktop/reproducibility'\n",
    "base_dir = 'Z:\\Marilyne\\Axioscan\\Gao_Zhang\\Segmentation'\n",
    "base_dir = '/Users/groutm/Desktop/gz_new'\n",
    "\n",
    "# Set name for of project\n",
    "# for use in directory creation\n",
    "project_name = 'ww'\n",
    "project_name = 'repro'\n",
    "project_name = 'gz_new'\n",
    "\n",
    "# Set string for current step, and for previous step\n",
    "# for use in file and direcotry naming\n",
    "step_suffix = 'bs'\n",
    "previous_step_suffix_long = \"_qc_eda\"\n",
    "\n",
    "# Initial input data directory\n",
    "#input_data_dir = r'/Users/groutm/Desktop/TMAdata'\n",
    "#input_data_dir = r'/Users/groutm/Desktop/ww_data'\n",
    "input_data_dir = os.path.join(base_dir, project_name + previous_step_suffix_long)\n",
    "\n",
    "# BS directory\n",
    "#output_data_dir = r'/Users/groutm/Desktop/TMAoutputdata'\n",
    "#output_data_dir = r'/Users/groutm/Desktop/ww_outputdata'\n",
    "output_data_dir = os.path.join(base_dir, project_name + \"_\" + step_suffix)\n",
    "\n",
    "# BS images subdirectory\n",
    "#output_images_dir = r'/Users/groutm/Desktop/TMAimages'\n",
    "#output_images_dir = r'/Users/groutm/Desktop/wwimages'\n",
    "output_images_dir = os.path.join(output_data_dir,\"images\")\n",
    "\n",
    "# Metadata directories\n",
    "metadata_dir = os.path.join(base_dir, project_name + \"_metadata\")\n",
    "metadata_images_dir = os.path.join(metadata_dir,\"images\")\n",
    "\n",
    "# Create necessary directories for this step, if they don't already exist\n",
    "for d in [base_dir, input_data_dir, output_data_dir, output_images_dir, \n",
    "          metadata_dir, metadata_images_dir]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "# Change directory to location of input files\n",
    "os.chdir(input_data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of samples for use in this step of workflow. Do not include file extensions or steps labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide list of samples whose files we want to read int\n",
    "# Needs to be a list of strings, which serve as bases for \n",
    "# input file names. Input files will be derived from base\n",
    "# sample names, previous step substring, and filetype \n",
    "# extension\n",
    "\n",
    "ls_samples = ['TMA','ww1', 'ww10', 'ww11', 'ww12', 'ww13', 'ww15', \n",
    "              'ww16', 'ww17', 'ww19', 'ww2', 'ww20', 'ww21', \n",
    "              'ww22', 'ww23', 'ww3', 'ww4', 'ww5', 'ww6', 'ww7', \n",
    "              'ww8', 'ww9']\n",
    "\n",
    "ls_samples = ['TMA1.1', 'TMA1.2', 'TMA1.3', 'TMA2.1', 'TMA2.2', 'TMA2.3']\n",
    "\n",
    "ls_samples = ['GZ10.1', 'GZ10.2', 'GZ10.3', 'TMA',\n",
    " 'GZ7.1', 'GZ6', 'GZ7.2']\n",
    "\n",
    "ls_samples = ['A_GZ2', 'B_GZ1', 'C_GZ5', 'D_GZ4', 'E_GZ3','F_GZ6','G_GZ7', 'H_GZ9','I_GZ10','TMA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all metadata we need from the QC/EDA chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"marker_intensity_metadata.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "metadata = pd.read_csv(filename)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, metadata.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# Verify headers are expected\n",
    "exp_cols = ['Round','Target','Channel','target_lower','full_column','marker','location']\n",
    "compare_headers(exp_cols, metadata.columns.values, \"Marker metadata file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some of dataframe - FYI\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### not_intensities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file should be a csv with the name of a single \"not intensity\" or \"not marker\" column (e.g., ROI_index, Nuc_X_Inv) on each line. No need for each item to actually be present in any dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"not_intensities.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "not_intensities = []\n",
    "with open(filename, 'r') as fh:\n",
    "    not_intensities = fh.read().strip().split(\"\\n\")\n",
    "    # take str, strip whitespace, split on new line character\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, not_intensities.shape[0])\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "\n",
    "# Print to console\n",
    "print(\"not_intensities = \")\n",
    "print(not_intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full_to_short_column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"full_to_short_column_names.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "\n",
    "# Turn into dictionary\n",
    "full_to_short_names = df.set_index('full_name').T.to_dict('records')[0]\n",
    "\n",
    "# Print information\n",
    "print('full_to_short_names =')\n",
    "print(full_to_short_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short_to_full_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"short_to_full_column_names.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "\n",
    "# Turn into dictionary\n",
    "short_to_full_names = df.set_index('short_name').T.to_dict('records')[0]\n",
    "\n",
    "# Print information\n",
    "print('short_to_full_names =')\n",
    "print(short_to_full_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sample_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "df = df.drop(columns = ['hex'])\n",
    "\n",
    "# our tuple of float values for rgb, (r, g, b) was read in \n",
    "# as a string '(r, g, b)'. We need to extract the r-, g-, and b-\n",
    "# substrings and convert them back into floats\n",
    "df['rgb'] = df.apply(lambda row: rgb_tuple_from_str(row['rgb']), axis = 1)\n",
    "\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "\n",
    "# Turn into dictionary\n",
    "sample_color_dict = df.set_index('Sample_ID').T.to_dict('rgb')[0]\n",
    "\n",
    "# Print information\n",
    "print('sample_color_dict =')\n",
    "print(sample_color_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"channel_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "df = df.drop(columns = ['hex'])\n",
    "\n",
    "# our tuple of float values for rgb, (r, g, b) was read in \n",
    "# as a string '(r, g, b)'. We need to extract the r-, g-, and b-\n",
    "# substrings and convert them back into floats\n",
    "df['rgb'] = df.apply(lambda row: rgb_tuple_from_str(row['rgb']), axis = 1)\n",
    "\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# Turn into dictionary\n",
    "channel_color_dict = df.set_index('Channel').T.to_dict('rgb')[0]\n",
    "\n",
    "# Print information\n",
    "print('channel_color_dict =')\n",
    "print(channel_color_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"round_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "df = df.drop(columns = ['hex'])\n",
    "\n",
    "# our tuple of float values for rgb, (r, g, b) was read in \n",
    "# as a string '(r, g, b)'. We need to extract the r-, g-, and b-\n",
    "# substrings and convert them back into floats\n",
    "df['rgb'] = df.apply(lambda row: rgb_tuple_from_str(row['rgb']), axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "# Turn into dictionary\n",
    "round_color_dict = df.set_index('Round').T.to_dict('rgb')[0]\n",
    "\n",
    "# Print information\n",
    "print('round_color_dict =')\n",
    "print(round_color_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the first row of the file correpsonding to the first sample (index = 0)\n",
    "# in ls_samples\n",
    "\n",
    "# We do not need to specify a directory, since we earlier changed\n",
    "# the current working directory to be that containing these files\n",
    "filename = ls_samples[0] + previous_step_suffix_long + \".csv\"\n",
    "\n",
    "# Read in only the first line\n",
    "df = pd.read_csv(filename, index_col = 0, nrows = 1)\n",
    "\n",
    "# Apply the changes to the headers as specified in above funciton\n",
    "df = apply_header_changes(df)\n",
    "\n",
    "# Set variable to hold default header values\n",
    "expected_headers = df.columns.values\n",
    "\n",
    "print(\"df index name is currently\",df.index.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Used \" + ls_samples[0] + previous_step_suffix_long +\n",
    "      \".csv to determine the expected, corrected headers for all files.\")\n",
    "print(\"There headers are: \\n\" + \", \".join([h for h in expected_headers]) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dictionary to hold all individual sample data\n",
    "dfs = {}\n",
    "\n",
    "# iterate through each sample in our list of samples\n",
    "for sample in ls_samples:\n",
    "    # Check for existence of file\n",
    "    if not os.path.exists(sample+previous_step_suffix_long+\".csv\"):\n",
    "        print(\"File \" + sample+previous_step_suffix_long+\".csv\" +\n",
    "             \" does not exist. Removing from analysis...\")\n",
    "        # Remove from list if not found\n",
    "        ls_samples.remove(sample)\n",
    "        continue\n",
    "        \n",
    "    # open the file\n",
    "    # set the index to be the first (0-based indexing, so 0th)\n",
    "    # column in input file.\n",
    "    df = pd.read_csv('{}.csv'.format(sample), index_col = 0)#,\n",
    "                    #nrows = 500) \n",
    "    # use nrows = # to specify number of input rows if you want\n",
    "    \n",
    "    # Check for empty df\n",
    "    # if so, don't continue trying to process df\n",
    "    if df.shape[0] == 0:\n",
    "        print('Zero content lines detected in ' + sample + ' file.'\n",
    "              'Removing from analysis...')\n",
    "        # Remove from list, so further steps won't be looking\n",
    "        # for data on this sample.\n",
    "        # Note that for lists, we do not need to re-assign\n",
    "        # the list when removing an item, i.e., we do not say\n",
    "        # 'ls_samples = ls_samples.remove(sample)', since this\n",
    "        # operation does not return anything.\n",
    "        ls_samples.remove(sample)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Verify that the loaded df are the right length\n",
    "    # commenting out because this code did not work on all\n",
    "    # machines during testing (failed one PC, succeeded with\n",
    "    # one PC and one MacBook)\n",
    "    try:\n",
    "        verify_line_no(sample + \".csv\", df.shape[0] + 1) \n",
    "    except:\n",
    "        pass\n",
    "    # adding 1 because we expect the header was detected \n",
    "    # during file import and not counted towards length of df\n",
    "    \n",
    "     # Manipulations necessary for concatenation\n",
    "    df = apply_header_changes(df)\n",
    "    # sort them alphanetically\n",
    "    df = df[[x for x in sorted(df.columns.values)]]\n",
    "    \n",
    "    \n",
    "    # Compare headers of new df against what is expected\n",
    "    compare_headers(expected_headers, df.columns.values, sample)\n",
    "    \n",
    "    \n",
    "    # For cases where we have samples called TMA1.1, TMA1.2, TMA1.3, etc.\n",
    "    # Using regular expressions (regex) to extract the characters in the\n",
    "    # sample name from TMA to the following digits, stopping at the period\n",
    "    #if 'ROI_index' in df.columns.values:\n",
    "     #   df['ROI_slide'] = re.findall(r'(TMA\\d+)',sample)[0]\n",
    "        \n",
    "    # Add to dictionary of dfs \n",
    "    dfs[sample] = df\n",
    "    \n",
    "\n",
    "\n",
    "#Merge dfs into one big df\n",
    "df = pd.concat(dfs.values(), ignore_index=False , sort = False)\n",
    "# remove dfs from memory, since its big (relatively) and we\n",
    "# don't need a data struture of all samples' data separated\n",
    "# individually when we can extract information from the big\n",
    "# df using the Sample_ID column\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few features to make sure our dataframe is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for NaN entries (should not be any unless columns do not align), which can result from stitching together dfs with different values in their headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are any null values, then print names of columns containing\n",
    "# null values\n",
    "if df.isnull().any().any():\n",
    "    print(df.columns[df.isnull().any()])\n",
    "\n",
    "#in 'if' statement, false means no NaN entries True means NaN entries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all expected files were imported into final dataframe by comparing our sample names to the unique values in the Sample_ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all expected files were imported into final dataframe\n",
    "\n",
    "if sorted(df.Sample_ID.unique()) == sorted(ls_samples):\n",
    "    print(\"All expected filenames present in big df Sample_ID column.\")\n",
    "else:\n",
    "    compare_headers(['no samples'], df.Sample_ID.unique(), \"big df Sample_ID column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter rows with 0 mean intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete rows that have 0 value mean intensities for intensity columns\n",
    "print(\"df.shape: \", df.shape)\n",
    "df = df.loc[df.apply(\n",
    "    lambda row: \n",
    "    row[~row.index.isin(not_intensities)].mean(), axis = 1) >0 ,:]\n",
    "\n",
    "print(\"df.shape: \", df.shape)\n",
    "# changed this from != 0 to 0\n",
    "# original line of code:\n",
    "#df = df.loc[((df.iloc[:,3:] != 0).all(1) )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R Shiny Filtering App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save file for filtering app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that the R Shiny apps work better with a smaller dataset. Here, I create a dataframe of only 10,000 rows, where the proportion of rows attributed to each sample is maintained, by setting 'original' in `create_subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_row_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = create_subset(\n",
    "    df = df, \n",
    "    col = 'Sample_ID', \n",
    "    count = subset_row_count, \n",
    "    ratio = 'original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lines for each sample ID are in our subset df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the proportions of cells in the original and subset dfs compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sample_ID'].value_counts().sort_index()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()/subset_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"for_filtering_app.csv\"\n",
    "filename = os.path.join(output_data_dir,filename)\n",
    "subset_df.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a reminder to us, here are the features available on which we can filter...\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter on nuc size and AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete small cells and objects w/high AF555 Signal (RBCs)\n",
    "df = df.loc[(df['Nucleus_Size'] > 60 )]\n",
    "df = df.loc[(df['Nucleus_Size'] < 600 )]\n",
    "print(\"Number of cells after filtering on nucleus size:\", df.shape[0])\n",
    "\n",
    "#df = df.loc[(df['AF555_Cytoplasm_Intensity_Average'] < 2500)]\n",
    "#df = df.loc[(df['AF555_Nucleus'] < 3000)]\n",
    "df = df.loc[(df['AF555_Cell_Intensity_Average'] < 2500)]\n",
    "print(\"Number of cells after filtering on AF555 ___ intensity:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign tumor cells \n",
    "\n",
    "def assign_cell_type(row):\n",
    "    print(row.keys)\n",
    "    if (row['CKs_Cytoplasm_Intensity_Average'] > 2000)  \\\n",
    "        | (row['Ecad_Cytoplasm_Intensity_Average'] > 1800) \\\n",
    "        | (row['NCad_Cytoplasm_Intensity_Average'] > 2200):\n",
    "            return 'TUMOR'\n",
    "    elif (row['CD45_Cytoplasm_Intensity_Average'] > 4500) \\\n",
    "     | (row['CD4_Cytoplasm_Intensity_Average'] > 3000) \\\n",
    "     | (row['CD68_Cytoplasm_Intensity_Average'] > 3200):\n",
    "        return 'IMMUNE'\n",
    "    else:\n",
    "        return 'STROMA'\n",
    "    \n",
    "###############\n",
    "### WARNING ###\n",
    "###############\n",
    "# randomly assigning here just for development purposes\n",
    "# comment out this version of assign_cell_types() when\n",
    "# running the code 'for real'\n",
    "def assign_cell_type(row):\n",
    "    n = np.random.randint(0,10)\n",
    "    if n < 2:\n",
    "        return 'IMMUNE'\n",
    "    if n < 5:\n",
    "        return 'CANCER'    \n",
    "    return 'STROMA'\n",
    "\n",
    "# First create a cell_type column and make it blank\n",
    "df['cell_type'] = ''\n",
    "\n",
    "# Then iterate through each sample and perform the cell type assignment as necessary\n",
    "my_list = [] # put sample names here\n",
    "my_list = ls_samples.copy() # comment this line out\n",
    "for sample in my_list:\n",
    "    df.loc[df['Sample_ID'] == sample,'cell_type'] = df.apply(lambda row: assign_cell_type(row), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For saving parameters to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe to hold parameters\n",
    "\n",
    "my_cols = ['Sample_ID','a','b']\n",
    "filtering_params = pd.DataFrame(columns = my_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your param info\n",
    "filtering_params = filtering_params.append(\n",
    "    pd.DataFrame(\n",
    "        {'Sample_ID':['my sample'],\n",
    "         'a':['parameter value'],\n",
    "        'b':['parameter value']}), sort = True)\n",
    "\n",
    "filtering_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file\n",
    "\n",
    "fileaname = \"filtering_params.csv\"\n",
    "filename = os.path.join(output_data_dir, filename)\n",
    "filtering_params.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save counts of each cell type to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Sample_ID, by cell type\n",
    "cell_series = df.groupby('Sample_ID').cell_type.value_counts()\n",
    "filename = \"counts_by_sample_ID_by_cell_type.csv\"\n",
    "filename = os.path.join(output_data_dir, filename)\n",
    "cell_series.to_csv(filename, header = True)\n",
    "print(\"Cell counts by Sample_ID, by cell type:\")\n",
    "print(cell_series)\n",
    "\n",
    "# Just by cell type\n",
    "cell_series = df.cell_type.value_counts()\n",
    "filename = \"counts_by_cell_type.csv\"\n",
    "filename = os.path.join(output_data_dir, filename)\n",
    "cell_series.to_csv(filename, header = True)\n",
    "print(\"Cell counts by cell type:\")\n",
    "print(cell_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish colors to use throughout workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell type - want colors that are categorical, since Cell Type is a non-ordered category. A categorical color palette will have dissimilar colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get those unique colors\n",
    "cell_types = ['STROMA','CANCER','IMMUNE']\n",
    "color_values = sb.color_palette(\"hls\",n_colors = len(cell_types))#'HLS'\n",
    "# each color value is a tuple of three values: (R, G, B)\n",
    "\n",
    "print(\"Unique cell types are:\",df.cell_type.unique())\n",
    "# Display those unique colors\n",
    "sb.palplot(sb.color_palette(color_values))\n",
    "\n",
    "# allow for user-input of named colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltype_color_dict = dict(zip(cell_types, color_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celltype_color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is an example of how you might specify the values yourself,\n",
    "# derived from the QC/EDA chapter\n",
    "\n",
    "#celltype_color_dict['CANCER'] = mplc.to_rgb('xkcd:dark sky blue')\n",
    "#celltype_color_dict['IMMUNE'] = mplc.to_rgb('xkcd:reddish orange')\n",
    "#celltype_color_dict['STROMA'] = mplc.to_rgb('xkcd:jungle green')\n",
    "\n",
    "sb.palplot(sb.color_palette(\n",
    "    [celltype_color_dict['IMMUNE'],\n",
    "     celltype_color_dict['STROMA'],\n",
    "     celltype_color_dict['CANCER']]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save color information (mapping and legend) to metadata directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "celltype_color_df = color_dict_to_df(celltype_color_dict, \"cell_type\")\n",
    "celltype_color_df.head()\n",
    "\n",
    "# Save to file in metadatadirectory\n",
    "filename = \"celltype_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "celltype_color_df.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legend of cell type info only\n",
    "\n",
    "g  = plt.figure(figsize = (1,1)).add_subplot(111)\n",
    "g.axis('off')\n",
    "handles = []\n",
    "for item in celltype_color_dict.keys():\n",
    "        h = g.bar(0,0, color = celltype_color_dict[item],\n",
    "                  label = item, linewidth =0)\n",
    "        handles.append(h)\n",
    "first_legend = plt.legend(handles=handles, loc='upper right', title = 'Cell type'),\n",
    "\n",
    "\n",
    "filename = \"Celltype_legend.png\"\n",
    "filename = os.path.join(metadata_images_dir, filename)\n",
    "plt.savefig(filename, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the post-filtering nucleus sizes, colored by cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_distr_plot_per_sample(title = \"Post-filtering nucleus sizes\",\n",
    "                           dfs = [df.loc[df['cell_type'] == 'STROMA',:],\n",
    "                               df.loc[df['cell_type'] == 'IMMUNE',:],\n",
    "                                  df.loc[df['cell_type'] == 'CANCER',:]], \n",
    "                           df_names = ['Immune','Cancer','Stroma'], \n",
    "                           colors = [celltype_color_dict['STROMA'],\n",
    "                               celltype_color_dict['IMMUNE'],\n",
    "                                    celltype_color_dict['CANCER']\n",
    "                                    ], \n",
    "                           x_label = \"Nucleus Size\", \n",
    "                           legend = False,\n",
    "                           markers = ['Nucleus_Size'],\n",
    "                          location = output_images_dir)\n",
    "\n",
    "# note that the traces are layered on top of each other, so if the tallest trace is in front, \n",
    "# then the other two will be tinted accordingly. You can change around the order of the data\n",
    "# in the parameter dfs = [] above, but don't forget to also change the order of the colors = []\n",
    "# parameter accordingly!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts for each Sample_ID, sorted by Sample_ID\n",
    "sample_counts = pd.DataFrame(df.Sample_ID.value_counts()).sort_index()\n",
    "sample_counts = sample_counts.rename(columns = {'Sample_ID':'counts'})\n",
    "sample_counts['Sample_ID'] = sample_counts.index\n",
    "#counts['color'] = counts.apply(lambda row: color_dict[row['Sample_ID']], axis = 1)\n",
    "sample_counts\n",
    "\n",
    "# There should be a better way to do this with 'groupby' or something\n",
    "stroma_counts = pd.DataFrame({'stroma':\n",
    "    df.loc[\n",
    "        df['cell_type'] == 'STROMA',:].Sample_ID.value_counts()}).sort_index()\n",
    "\n",
    "immune_counts = pd.DataFrame({'immune':\n",
    "    df.loc[\n",
    "        df['cell_type'] == 'IMMUNE',:].Sample_ID.value_counts()}).sort_index()\n",
    "\n",
    "cancer_counts = pd.DataFrame({'cancer':\n",
    "    df.loc[\n",
    "        df['cell_type'] == 'CANCER',:].Sample_ID.value_counts()}).sort_index()\n",
    "\n",
    "counts = pd.concat([sample_counts, stroma_counts,cancer_counts,immune_counts],\n",
    "                   axis = 1, sort = False)\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eastablish the proportional breakdown of each cell type in dataframe for relevant plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perc(row, cell_type):\n",
    "    total = row['stroma'] + row['immune'] + row['cancer']\n",
    "    return round(row[cell_type]/total *100,1)\n",
    "\n",
    "counts['stroma_perc'] = counts.apply(lambda row: get_perc(row, 'stroma'), axis = 1)\n",
    "counts['immune_perc'] = counts.apply(lambda row: get_perc(row, 'immune'), axis = 1)\n",
    "counts['cancer_perc'] = counts.apply(lambda row: get_perc(row, 'cancer'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By sample ID only \n",
    "\n",
    "# Establish figure\n",
    "fig = go.Figure()\n",
    "title = 'Post-filtering BS Cell counts by Sample ID'\n",
    "\n",
    "# Iterate through all samples and add trace to plot\n",
    "for sample in ls_samples:\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=counts.loc[counts['Sample_ID']==sample,'Sample_ID'], \n",
    "        y = counts.loc[counts['Sample_ID']==sample,'counts'],\n",
    "        text = counts.loc[counts['Sample_ID']==sample,'counts'], textposition='outside',\n",
    "        marker=dict(\n",
    "            color='rgb' + str(sample_color_dict[sample])),\n",
    "            showlegend = False\n",
    "        \n",
    "    ))\n",
    "    \n",
    "# Update aesthetic parameters\n",
    "fig.update_layout(title = title, plot_bgcolor = 'white')\n",
    "fig.update_xaxes(title_text = \"Sample ID\", linecolor = 'black')\n",
    "fig.update_yaxes(title_text = \"Cell count\", linecolor = \"black\")\n",
    "\n",
    "# Figure output\n",
    "#plot(fig)\n",
    "fig.write_image(output_images_dir + \"/\" + title.replace(\" \",\"_\") + \".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By sample ID and cell type - proportion\n",
    "\n",
    "# Establish figure\n",
    "fig = go.Figure()\n",
    "title = 'BS Cell proportions by Sample ID and tissue type'\n",
    "\n",
    "# Plot all three cell types in one go\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Stroma', x=counts['Sample_ID'], y=counts['stroma_perc'], \n",
    "           text = counts['stroma_perc'], textposition='auto',\n",
    "           marker_color = 'rgb' + str(celltype_color_dict['STROMA'])),\n",
    "    go.Bar(name='Immune', x=counts['Sample_ID'], y=counts['immune_perc'], \n",
    "           text = counts['immune_perc'], textposition='auto',\n",
    "           marker_color = 'rgb' + str(celltype_color_dict['IMMUNE'])),\n",
    "    go.Bar(name='Cancer',x=counts['Sample_ID'], y=counts['cancer_perc'], \n",
    "           text = counts['cancer_perc'], textposition='auto', \n",
    "           marker_color = 'rgb' + str(celltype_color_dict['CANCER']))\n",
    "])\n",
    "\n",
    "# Adjust aesthetic parameters\n",
    "fig.update_layout(title = title, plot_bgcolor = 'white',barmode ='stack')\n",
    "fig.update_xaxes(title = \"Sample\", linecolor = 'black')\n",
    "fig.update_yaxes(title = \"Cell count\", linecolor = 'black')\n",
    "\n",
    "# Figure output\n",
    "#plot(fig)\n",
    "fig.write_image(output_images_dir + \"/\" + title.replace(\" \",\"_\") + \".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By sample ID and cell type - count\n",
    "\n",
    "# Establish figure\n",
    "fig = go.Figure()\n",
    "title = 'BS Cell counts by Sample ID and tissue type'\n",
    "\n",
    "# Plot all three cell types in one go\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Stroma', x=counts['Sample_ID'], y=counts['stroma'], \n",
    "           text = counts['stroma'], textposition='auto',\n",
    "           marker_color = 'rgb' + str(celltype_color_dict['STROMA'])),\n",
    "    go.Bar(name='Immune', x=counts['Sample_ID'], y=counts['immune'], \n",
    "           text = counts['immune'], textposition='auto',\n",
    "           marker_color = 'rgb' + str(celltype_color_dict['IMMUNE'])),\n",
    "    go.Bar(name='Cancer',x=counts['Sample_ID'], y=counts['cancer'], \n",
    "           text = counts['cancer'], textposition='auto', \n",
    "           marker_color = 'rgb' + str(celltype_color_dict['CANCER']))\n",
    "])\n",
    " \n",
    "# Adjust aesthetic parameters\n",
    "fig.update_layout(title = title, plot_bgcolor = 'white',barmode ='stack')\n",
    "fig.update_xaxes(title = \"Sample\", linecolor = 'black')\n",
    "fig.update_yaxes(title = \"Cell count\", linecolor = 'black')\n",
    "\n",
    "# Figure output\n",
    "#plot(fig)\n",
    "fig.write_image(output_images_dir + \"/\" + title.replace(\" \",\"_\") + \".png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue with BS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divide each marker (and its location) by the right exposure setting for each group of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_exp_time(col, exp_col):\n",
    "    exp_time = metadata.loc[metadata['full_column'] == col.name, exp_col].values[0]\n",
    "    return col/exp_time\n",
    "\n",
    "#df.loc[df['ROI_slide'] == 'TMA1',~df.columns.isin(not_intensities)] = \\\n",
    "#    df.loc[df['ROI_slide'] == 'TMA1',~df.columns.isin(not_intensities)].apply(\n",
    "#    lambda column: divide_exp_time(column, 'Exp_TMA1'), axis = 0)\n",
    "\n",
    "#df.loc[df['ROI_slide'] == 'TMA2',~df.columns.isin(not_intensities)] = \\\n",
    "#    df.loc[df['ROI_slide'] == 'TMA2',~df.columns.isin(not_intensities)].apply(\n",
    "#    lambda column: divide_exp_time(column, 'Exp_TMA2'), axis = 0)\n",
    "\n",
    "# Operate only on not_intensity columns\n",
    "# Divide each of these columns by the appropriate exposure time in the metadata dataframe\n",
    "df.loc[:, ~df.columns.isin(not_intensities)] = \\\n",
    "    df.loc[:, ~df.columns.isin(not_intensities)].apply(\n",
    "    lambda column: divide_exp_time(column, 'Exp'), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do background subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata.to_csv(\"/Users/groutm/Desktop/metadata_for_PCA_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform background substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_background_sub(col):\n",
    "    #print(col.name)\n",
    "    location = metadata.loc[metadata['full_column'] == col.name, 'location'].values[0]\n",
    "    #print('location = ' + location)\n",
    "    channel = metadata.loc[metadata['full_column'] == col.name, 'Channel'].values[0]\n",
    "    #print('channel = ' + channel)\n",
    "    af_target = metadata.loc[\n",
    "        (metadata['Channel']==channel) \\\n",
    "        & (metadata['location']==location) \\\n",
    "        &(metadata['target_lower'].str.contains(r'^af\\d{3}$')),\\\n",
    "        'full_column'].values[0]\n",
    "    # ^ right channel, right location, right maker (AF baseline)\n",
    "    # don't want to subtract an AF channel from itself\n",
    "    if af_target == col.name:\n",
    "        return col\n",
    "    return col - df.loc[:,af_target]\n",
    "\n",
    "\n",
    "# Operate only on not_intensity columns\n",
    "# Subtract from non-AF columns the appropriate AF column value for that cell.\n",
    "df.loc[:,~df.columns.isin(not_intensities)] = \\\n",
    "    df.loc[:,~df.columns.isin(not_intensities)].apply(\n",
    "    lambda column: do_background_sub(column), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers (replace outliers with X percentile)\n",
    "outlier_percent = 0.005\n",
    "\n",
    "# Establish our allowable ceiling value for each feature\n",
    "upper_lim = df.loc[\n",
    "    :,~df.columns.isin(not_intensities)].quantile(1 - outlier_percent)\n",
    "\n",
    "# Identify which values exceed allowable ceiling value\n",
    "upper_outliers = (df.loc[:,~df.columns.isin(not_intensities)] > upper_lim)\n",
    "\n",
    "# Set the exceeding value to the ceiling value\n",
    "df.loc[:,~df.columns.isin(not_intensities)] = \\\n",
    "    df.loc[:,~df.columns.isin(not_intensities)].mask(upper_outliers, upper_lim, axis=1)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diplay what the upper limits are for each marker\n",
    "upper_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set values < 0 to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set anything that is below 0 to 0, so that we can do the log transform\n",
    "for f in df.columns[~df.columns.isin(not_intensities)]:\n",
    "    df.loc[df[f] < 0,f] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop AF columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no use for AF columns after background subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(regex='^(?!AF\\d{3}).*')\n",
    "\n",
    "## annotate the regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further background subtraction visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only be plotting ~10k cells in the interest of time/computing resources. We want these 10k lines in our original df to be sampled randomly, without replacement, with the caveat that the proportions of all samples in the data remains the same in this subset. If the size of the dataframe is > 10k rows, then we will proceed with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_row_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = create_subset(df, 'Sample_ID', subset_row_count, 'equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lines for each sample ID are in our subset df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the proportions of cells in the original and subset dfs compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sample_ID'].value_counts().sort_index()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()/subset_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data structures to map colors to columns and rows..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the row colors, we essentially just need to map the information in a given feature to the colors that correspond to that value in the right color dictionary. For example, it might be sample_3, sample_3, sample_4, , so we need the row colors to be (1, 1, 1), (1, 1, 1), (0, 0.25, 0.6). These are the initialy colors--if we are clustering rows or columns, the labels will still match the data with which they're associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row_colors = subset_df.Sample_ID.map(sample_color_dict)\n",
    "\n",
    "sample_row_colors[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For column rows, matching up the information in each column with the appropriate color is more difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we want to translate marker columns to their corresponding channel information,\n",
    "# and then match that up with the right color, as with row columns\n",
    "\n",
    "# First, we merge the (L) non-intensity column values, transformed into a dataframe,\n",
    "# with the metadata df (R), matching on the \"0\" column present in the L,\n",
    "# which is the only column in there, with the \"full_column\" (aka df header name)\n",
    "# column in the R, only including all cases where there is a match and any unmatched\n",
    "# L cases ('both' [?] would be only cases where ther is is a match, and 'right' would\n",
    "# be cases with a match and any unmatched R columns).\n",
    "column_channel_colors = pd.merge(pd.DataFrame(pd.Series(\n",
    "    subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values)), \n",
    "                  metadata, how = 'left',\n",
    "         left_on = 0, right_on = 'full_column')[[0,'Channel']]['Channel'].map(channel_color_dict)\n",
    "\n",
    "# Set the index to be the names of the colors. There is only one column, and that is the corresponding\n",
    "# colors\n",
    "column_channel_colors.index = subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values\n",
    "\n",
    "column_channel_colors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we want to translate marker columns to their corresponding round information,\n",
    "# and then match that up with the right color, as with row columns\n",
    "\n",
    "# First, we merge the (L) non-intensity column values, transformed into a dataframe,\n",
    "# with the metadata df (R), matching on the \"0\" column present in the L,\n",
    "# which is the only column in there, with the \"full_column\" (aka df header name)\n",
    "# column in the R, only including all cases where there is a match and any unmatched\n",
    "# L cases ('both' [?] would be only cases where ther is is a match, and 'right' would\n",
    "# be cases with a match and any unmatched R columns).\n",
    "column_round_colors = pd.merge(pd.DataFrame(pd.Series(\n",
    "    subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values)), \n",
    "                  metadata, how = 'left',\n",
    "         left_on = 0, right_on = 'full_column')[[0,'Round']]['Round'].map(round_color_dict)\n",
    "\n",
    "# Set the index to be the names of the colors. There is only one column, and that is the corresponding\n",
    "# colors\n",
    "column_round_colors.index = subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values\n",
    "\n",
    "column_round_colors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data structure to hold everything we need for row/column annotations\n",
    "# annotations is a dictionary\n",
    "## IMPORTANT - if you use 'annotations', it MUST have both 'rows' and 'cols'\n",
    "## objects inside. These can be empty lists, but they must be there!\n",
    "annotations = {}\n",
    "\n",
    "# create a data structure to hold everything we need for only row annotations\n",
    "# row_annotations is a list, where each item therein is a dictioary corresponding\n",
    "# to all of the data pertaining to that particular annotation\n",
    "# Adding each item (e.g., Sample, then Cluster), one at a time to ensure ordering\n",
    "# is as anticipated on figure\n",
    "row_annotations = []\n",
    "row_annotations.append({'label':'Sample','type':'row','mapping':sample_row_colors,'dict':sample_color_dict,\n",
    "                        'location':'center left','bbox_to_anchor':(0, 0.5)})\n",
    "# Add all row information into the annotations dictionary\n",
    "annotations['rows'] = row_annotations\n",
    "\n",
    "\n",
    "# Now we repeat the process for column annotations\n",
    "col_annotations = []\n",
    "col_annotations.append({'label':'Round','type':'column','mapping':column_round_colors,'dict':round_color_dict,\n",
    "                       'location':'upper right','bbox_to_anchor':(1,0.50)})\n",
    "\n",
    "col_annotations.append({'label':'Column','type':'column','mapping':column_channel_colors,'dict':channel_color_dict,\n",
    "                       'location':'upper right','bbox_to_anchor':(1,0.75)})\n",
    "annotations['cols'] = col_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually plot the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_function(\n",
    "    data = subset_df.loc[:,~subset_df.columns.isin(not_intensities)],\n",
    "    title = \"Background substraction heatmap\",\n",
    "    # define method, metric, and color map\n",
    "    method = 'ward', metric = 'euclidean',cmap = 'coolwarm',\n",
    "    # colorbar (legend coloring of main plot) \n",
    "    cbar_kws = {'label':'BS Intens.'},\n",
    "    # xticklabels - want to have the nicknames instead of full names,\n",
    "    # so we translate from full to short names; we also only want to include\n",
    "    # non_intensity columns, to match the data we fed into under 'data'\n",
    "    xticklabels = [full_to_short_names[name] for name in \n",
    "                     subset_df.loc[:,\n",
    "                                 ~subset_df.columns.isin(not_intensities)].columns.values],\n",
    "    # where to save the dataframe        \n",
    "    save_loc = output_images_dir,\n",
    "    # Boolean values for clustering\n",
    "    row_cluster = True, col_cluster = True,\n",
    "    # provide annotations established aboved\n",
    "    annotations = annotations\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #### XY plot - one per sample, colored by cell type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed to make a subset of the original df, since the original was too big for my computer to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_row_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = create_subset(df, 'Sample_ID', subset_row_count, 'original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lines for each sample ID are in our subset df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the proportions of cells in the original and subset dfs compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sample_ID'].value_counts().sort_index()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()/subset_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all samples\n",
    "\n",
    "x_feature = 'Nuc_X'\n",
    "y_feature = 'Nuc_Y_Inv'\n",
    "\n",
    "for sample in ls_samples:\n",
    "    # Extract x/y and cell type infor for all cells\n",
    "    location_colors = subset_df.loc[subset_df['Sample_ID'] == sample,[x_feature,y_feature,'cell_type']]\n",
    "\n",
    "    # Establish figure\n",
    "    fig = go.Figure()\n",
    "    title = sample + \" Background Subtracted XY Map\"\n",
    "\n",
    "    # Iterate through cell types and plot each trace\n",
    "    for celltype in subset_df.loc[subset_df['Sample_ID'] == sample,'cell_type'].unique():\n",
    "        fig.add_scatter(\n",
    "            # Only want\n",
    "            mode = 'markers',\n",
    "            marker=dict(\n",
    "                        size=2, opacity = 0.70, # size is dot size, higher opacity = less opaque\n",
    "                        color='rgb' + str(celltype_color_dict[celltype])#,\n",
    "                #line = dict(width = 2, color = 'gray') # line around each marker\n",
    "                       ),\n",
    "        x = location_colors.loc[location_colors['cell_type']==celltype,x_feature],\n",
    "        y = location_colors.loc[location_colors['cell_type']==celltype,y_feature],\n",
    "        name = celltype)\n",
    "\n",
    "    # Update aesthetic parameters\n",
    "    fig.update_layout(title = title, plot_bgcolor = 'white',\n",
    "                     legend= {'itemsizing': 'constant'}) # make the legend dots a bit bigger    \n",
    "    fig.update_xaxes(title_text = x_feature, linecolor = 'black')\n",
    "    fig.update_yaxes(title_text = y_feature, linecolor = 'black')\n",
    "\n",
    "    # Figure output\n",
    "    #plot(fig)\n",
    "    fig.write_image(output_images_dir + \"/\" + title.replace(\" \",\"_\") + \".png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R Shiny - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here might me a good place to perform PCA. You can save an output file like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"for_PCA.csv\"\n",
    "filename = os.path.join(output_data_dir, filename)\n",
    "subset_df.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One per sample, one per marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-sample, per-marker distribution plots\n",
    "\n",
    "df = df.rename(columns = full_to_short_names)\n",
    "\n",
    "for sample in ls_samples:\n",
    "#for sample in [s for s in ls_samples if df.loc[df['Sample_ID'] == s,:].shape[0] > 0]:\n",
    "#for sample in ['ww10','TMA','ww12']:\n",
    "    for marker in [m for m in df.columns.values if m not in not_intensities]:\n",
    "        make_distr_plot_per_sample(\n",
    "        title = sample + \" \" + marker + \" BS\",\n",
    "        dfs = [\n",
    "            df.loc[df['Sample_ID']==sample,:].copy()],\n",
    "        df_names = [sample],\n",
    "        colors = [sample_color_dict[sample]],\n",
    "        x_label = \"Intensity\",\n",
    "        legend = False,\n",
    "        markers = marker,\n",
    "        location = output_images_dir)     \n",
    "\n",
    "df = df.rename(columns = short_to_full_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions - one per channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All samples are represented individually on each plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.loc[ (metadata['target_lower'].str.contains(r'^af\\d{3}$')), ['Target','Channel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in metadata.Channel.unique():\n",
    "    my_cols = metadata.loc[metadata['Channel'] == c, 'full_column']\n",
    "    channel_data = df.loc[:,[f for f in my_cols if f in df.columns]]\n",
    "    channel_data = channel_data.rename(columns = full_to_short_names)\n",
    "    wavelength = metadata.loc[(metadata['Channel'] == c) \\\n",
    "                              & (metadata['target_lower'].str.contains(r'^af\\d{3}$')), 'Target'].values[0]\n",
    "    make_distr_plot_per_sample(\n",
    "        title = \"Distribution of \" + wavelength + \" markers across all BS cells\",\n",
    "        dfs = [channel_data],\n",
    "            df_names = [''],\n",
    "                              colors = sb.color_palette(\"hls\",n_colors = channel_data.shape[1]),\n",
    "                              x_label = \"Intensity\",\n",
    "                              legend = True,\n",
    "    location = output_images_dir, not_intensities = not_intensities)\n",
    "\n",
    "## make sure it's clear how to set xlims manually for these plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot - cell size vs nucleus size, color = nulceus roundess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful now that we have performed filtering. Note that real workflow will use the appropriate features that are lacking in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[df.columns.isin(not_intensities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish labeling\n",
    "title = \"Nucleus size by cell size for post-filtering data\"\n",
    "x_label = \"Nucleus Size\"\n",
    "y_label = \"Cell Size\" # cell size\n",
    "\n",
    "# Create plot\n",
    "fig = px.scatter(subset_df, x=\"Cell_Size\", y=\"Nucleus_Size\",\n",
    "                 color='Nucleus_Roundness')\n",
    "                 \n",
    "fig.update_layout(title_text=title, font=dict(size=18), \n",
    "        plot_bgcolor = 'white', showlegend = True )\n",
    "\n",
    "# Adjust opacity of traces and size of marker\n",
    "fig.update_traces(opacity=0.6, marker ={'size':5})\n",
    "# Adjust x-axis parameters\n",
    "fig.update_xaxes(title_text = x_label, showline=True, linewidth=2, linecolor='black', \n",
    "        tickfont=dict(size=18))\n",
    "    # Adjust y-axis parameters\n",
    "fig.update_yaxes(title_text = y_label, showline=True, linewidth=2, linecolor='black',\n",
    "        tickfont=dict(size=18))\n",
    "\n",
    "# Figure output\n",
    "#plot(fig)\n",
    "filename = title.replace(\" \",\"_\") + \".png\"\n",
    "filename = os.path.join(output_images_dir, filename)\n",
    "fig.write_image(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any other rows or columns we want to before saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of example, I will operate on a copy of df, called df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST do df.copy()\n",
    "# 'df_copy = df' would essentially \n",
    "# give you two different names for the\n",
    "# SAME dataframe, so operating on one\n",
    "# would also operate on the other\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operate on entire rows or columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "my_cols = []\n",
    "df_copy = df_copy.drop(columns = my_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only specific columns (explained below)\n",
    "my_cols = []\n",
    "my_cols = df.columns.values\n",
    "df_copy = df_copy.loc[:,my_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operate on rows and columns using filtering criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only certain rows based off of criteria\n",
    "\n",
    "# use df.loc[] to filter\n",
    "# df.loc[rows,columns]\n",
    "# df.loc[:,certain_cols] --> keep all rows ':', only certain cols\n",
    "# df.loc[certain_rows,:] --> keep only certain row, all cols ':'\n",
    "\n",
    "# Say we only want certain values for Sample_ID\n",
    "print(df_copy.Sample_ID.unique())\n",
    "keep = ['TMA1.1','TMA1.2','TMA1.3','TMA2.1','TMA2.2','TMA2.3']\n",
    "df_copy = df_copy.loc[df_copy['Sample_ID'].isin(keep),:]\n",
    "print(df_copy.Sample_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on multiple criteria\n",
    "# '&' or 'and'\n",
    "# '|' or 'or'\n",
    "# you MUST have parentheses around each logic expression!\n",
    "df_copy = df_copy.loc[\n",
    "    (df_copy['Sample_ID'].isin(['TMA1.1','TMA1.2','TMA1.3'])) \\\n",
    "    ## backslash above used to break line for readability, but tell Python to act like it's all one line\n",
    "        | (df_copy['Sample_ID'].isin(['TMA2.1','TMA2.2','TMA2.3'])),:]\n",
    "print(df_copy.Sample_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows based off of certain criteria\n",
    "# note the negating tilde '~'!\n",
    "\n",
    "df_copy = df_copy.loc[\n",
    "    (~df_copy['Sample_ID'].isin(['TMA1.1','TMA1.2','TMA1.3'])) \\\n",
    "    ## backslash above used to break line for readability, but tell Python to act like it's all one line\n",
    "        & (~df_copy['Sample_ID'].isin(['TMA2.1','TMA2.2','TMA2.3'])),:]\n",
    "print(df_copy.Sample_ID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data by Sample_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of output file first\n",
    "for sample in ls_samples:\n",
    "    filename = os.path.join(output_data_dir+ \"/\" + sample + \"_\" + step_suffix + \".csv\")\n",
    "    if os.path.exists(filename):\n",
    "        print(\"File by name \"+filename+\" already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output files\n",
    "for sample in ls_samples:\n",
    "    df_save = df.loc[df['Sample_ID'] == sample,:]\n",
    "    filename = os.path.join(output_data_dir,   sample + \"_\" + step_suffix + \".csv\")\n",
    "    df_save.to_csv(filename, index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

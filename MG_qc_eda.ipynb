{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality control/Exploratory data analysis Notebook\n",
    "\n",
    "By: Megan Grout (groutm@ohsu.edu)\n",
    "\n",
    "Adapted from code written by Dr. Marilyne Labrie and Nick Kendsersky\n",
    "\n",
    "\n",
    "Last updated: 20191219"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import external libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mplc\n",
    "import subprocess\n",
    "\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \n",
    "import plotly.express as px\n",
    "init_notebook_mode(connected = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import function written for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycif_modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to change header names. Not encapsutated in `cycif_modules`, so that user can change on the fly as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may change for each experiment, so I have not sequestered\n",
    "# this code in the cycif_modules.py file\n",
    "\n",
    "# This function takes in a dataframe, changes the names\n",
    "# of the column in various ways, and returns the dataframe.\n",
    "# For best accuracy and generalizability, the code uses\n",
    "# regular expressions (regex) to find strings for replacement.\n",
    "def apply_header_changes(df):\n",
    "    # remove lowercase x at beginning of name\n",
    "    df.columns = df.columns.str.replace(\"^x\",\"\")\n",
    "    # remove space at beginning of name\n",
    "    df.columns = df.columns.str.replace(\"^ \",\"\")\n",
    "    # replace space with underscore\n",
    "    df.columns = df.columns.str.replace(\" \",\"_\")\n",
    "    # fix typos\n",
    "    df.columns = df.columns.str.replace(\"CKD1\",\"CDK1\")\n",
    "    df.columns = df.columns.str.replace(\"GAG3\",\"GATA3\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for project\n",
    "base_dir = '/Users/groutm/Desktop/weewin'\n",
    "base_dir = '/Users/groutm/Desktop/reproducibility'\n",
    "base_dir = 'Z:\\Marilyne\\Axioscan\\Gao_Zhang\\Segmentation'\n",
    "base_dir = '/Users/groutm/Desktop/gz_new'\n",
    "\n",
    "# Set name for of project\n",
    "# for use in directory creation\n",
    "project_name = 'ww'\n",
    "project_name = 'repro'\n",
    "project_name = 'gz_new'\n",
    "\n",
    "# Set string for current step, and for previous step\n",
    "# for use in file and direcotry naming\n",
    "step_suffix = 'bs'\n",
    "previous_step_suffix_long = \"_qc_eda\"\n",
    "\n",
    "# Initial input data directory\n",
    "#input_data_dir = r'/Users/groutm/Desktop/TMAdata'\n",
    "#input_data_dir = r'/Users/groutm/Desktop/ww_data'\n",
    "input_data_dir = os.path.join(base_dir, project_name + previous_step_suffix_long)\n",
    "\n",
    "\n",
    "# BS directory\n",
    "#output_data_dir = r'/Users/groutm/Desktop/TMAoutputdata'\n",
    "#output_data_dir = r'/Users/groutm/Desktop/ww_outputdata'\n",
    "output_data_dir = os.path.join(base_dir, project_name + \"_\" + step_suffix)\n",
    "\n",
    "# BS images subdirectory\n",
    "#output_images_dir = r'/Users/groutm/Desktop/TMAimages'\n",
    "#output_images_dir = r'/Users/groutm/Desktop/wwimages'\n",
    "output_images_dir = os.path.join(output_data_dir,\"images\")\n",
    "\n",
    "# Metadata directories\n",
    "metadata_dir = os.path.join(base_dir, project_name + \"_metadata\")\n",
    "metadata_images_dir = os.path.join(metadata_dir,\"images\")\n",
    "\n",
    "# Create necessary directories for this step, if they don't already exist\n",
    "for d in [base_dir, input_data_dir, output_data_dir, output_images_dir, \n",
    "          metadata_dir, metadata_images_dir]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "# Change directory to location of input files\n",
    "os.chdir(input_data_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of samples for use in this step of workflow. Do not include file extensions or steps labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide list of samples whose files we want to read int\n",
    "# Needs to be a list of strings, which serve as bases for \n",
    "# input file names. Input files will be derived from base\n",
    "# sample names, previous step substring, and filetype \n",
    "# extension\n",
    "\n",
    "ls_samples = ['TMA','ww1', 'ww10', 'ww11', 'ww12', 'ww13', 'ww15', \n",
    "              'ww16', 'ww17', 'ww19', 'ww2', 'ww20', 'ww21', \n",
    "              'ww22', 'ww23', 'ww3', 'ww4', 'ww5', 'ww6', 'ww7', \n",
    "              'ww8', 'ww9']\n",
    "\n",
    "ls_samples = ['TMA1.1', 'TMA1.2', 'TMA1.3', 'TMA2.1', 'TMA2.2', 'TMA2.3']\n",
    "\n",
    "ls_samples = ['GZ10.1', 'GZ10.2', 'GZ10.3', 'TMA',\n",
    " 'GZ7.1', 'GZ6', 'GZ7.2']\n",
    "\n",
    "ls_samples = ['A_GZ2', 'B_GZ1', 'C_GZ5', 'D_GZ4', 'E_GZ3','F_GZ6','G_GZ7', 'H_GZ9','I_GZ10','TMA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of columns that are not marker intensities. It is okay if any of these are not actually present in a given dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_intensities = ['replicate_ID', 'cell_type', 'Nucleus_Roundness', 'Nucleus_Size', 'Cell_Size',\n",
    "                   'Nuc_X', 'not','Nuc_X_Inv','Cell_ID','Nuc_Y_Inv','ROI_slide','ROI_index','Nuc_Y',\n",
    "                   'cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import segmentation files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, ascertain header of first sample's input file. This information will be used as a template against which all other input data files' headers will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the first row of the file correpsonding to the first sample (index = 0)\n",
    "# in ls_samples\n",
    "\n",
    "# We do not need to specify a directory, since we earlier changed\n",
    "# the current working directory to be that containing these files\n",
    "filename = ls_samples[0] + previous_step_suffix_long + \".csv\"\n",
    "\n",
    "# Read in only the first line\n",
    "df = pd.read_csv(filename, index_col = 0, nrows = 1)\n",
    "\n",
    "# Verify that the ID column in input file became the index\n",
    "# For segmentation files, we need the first column to be the \n",
    "# cell index. In later steps, the cell index will actually not\n",
    "# be a proper dataframe data column, but the index of the saved\n",
    "# dataframe from the previous step.\n",
    "if df.index.name != \"ID\":\n",
    "    print(\"Expected the first column in input file (index_col = 0) \"\n",
    "         \"to be 'ID'. This column will be used to set the index names\"\n",
    "         \"(cell number for each sample). It appears that the column '\"\n",
    "         + df.index.name + \"' was actually the imported as the index \"\n",
    "         \"column.\")\n",
    "\n",
    "# Apply the changes to the headers as specified in above funciton\n",
    "df = apply_header_changes(df)\n",
    "\n",
    "# Set variable to hold default header values\n",
    "expected_headers = df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this entry point into the workflow, we expect the first column to be the ID index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI - What are the headers in our dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Used \" + ls_samples[0] + \".csv to determine the expected, corrected headers for all files.\")\n",
    "print(\"There headers are: \\n\" + \", \".join([h for h in expected_headers]) + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import segmentation files for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dictionary to hold all individual sample data\n",
    "dfs = {}\n",
    "\n",
    "# iterate through each sample in our list of samples\n",
    "for sample in ls_samples:\n",
    "    # open the file\n",
    "    # set the index to be the first (0-based indexing, so 0th)\n",
    "    # column in input file.\n",
    "    df = pd.read_csv('{}.csv'.format(sample), index_col = 0)#,\n",
    "                    #nrows = 500) \n",
    "    # use nrows = # to specify number of input rows if you want\n",
    "    \n",
    "    # Check for empty df\n",
    "    # if so, don't continue trying to process df\n",
    "    if df.shape[0] == 0:\n",
    "        print('Zero content lines detected in ' + sample + ' file.'\n",
    "              'Removing from analysis...')\n",
    "        # Remove from list, so further steps won't be looking\n",
    "        # for data on this sample.\n",
    "        # Note that for lists, we do not need to re-assign\n",
    "        # the list when removing an item, i.e., we do not say\n",
    "        # 'ls_samples = ls_samples.remove(sample)', since this\n",
    "        # operation does not return anything.\n",
    "        ls_samples.remove(sample)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Verify that the loaded df are the right length\n",
    "    # commenting out because this code did not work on all\n",
    "    # machines during testing (failed one PC, succeeded with\n",
    "    # one PC and one MacBook)\n",
    "    try:\n",
    "        verify_line_no(sample + \".csv\", df.shape[0] + 1) \n",
    "    except:\n",
    "        pass\n",
    "    # adding 1 because we expect the header was detected \n",
    "    # during file import and not counted towards length of df\n",
    "    \n",
    "     # Manipulations necessary for concatenation\n",
    "    df = apply_header_changes(df)\n",
    "    # sort them alphanetically\n",
    "    df = df[[x for x in sorted(df.columns.values)]]\n",
    "    \n",
    "    \n",
    "    # Compare headers of new df against what is expected\n",
    "    compare_headers(expected_headers, df.columns.values, sample)\n",
    "    \n",
    "    # Add Sample_ID column and set it equal to sample name for sample\n",
    "    df['Sample_ID'] = sample\n",
    "    \n",
    "    \n",
    "    \n",
    "    # For cases where we have samples called TMA1.1, TMA1.2, TMA1.3, etc.\n",
    "    # Using regular expressions (regex) to extract the characters in the\n",
    "    # sample name from TMA to the following digits, stopping at the period\n",
    "    #if 'ROI_index' in df.columns.values:\n",
    "     #   df['ROI_slide'] = re.findall(r'(TMA\\d+)',sample)[0]\n",
    "        \n",
    "    # Add to dictionary of dfs \n",
    "    dfs[sample] = df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#Merge dfs into one big df\n",
    "df = pd.concat(dfs.values(), ignore_index=False , sort = False)\n",
    "# remove dfs from memory, since its big (relatively) and we\n",
    "# don't need a data struture of all samples' data separated\n",
    "# individually when we can extract information from the big\n",
    "# df using the Sample_ID column\n",
    "del dfs\n",
    "\n",
    "# set index to Sample_ID + cell number\n",
    "df = df.copy().reset_index(drop=True)\n",
    "index = []\n",
    "# Iterate through each sample, and extract from the big\n",
    "# df just the rows corresponding to that sample. Then, \n",
    "# reassign the cell index based off of the Sample_ID value\n",
    "# and the row number within that chunk. Save that information\n",
    "# in a list of indices\n",
    "for sample in ls_samples:\n",
    "    df_chunk = df.loc[df['Sample_ID'] == sample,:].copy()\n",
    "    old_index = df_chunk.index\n",
    "    df_chunk = df_chunk.reset_index(drop=True)\n",
    "    df_chunk = df_chunk.set_index(f'{sample}_Cell_' + df_chunk.index.astype(str))\n",
    "    index = index + df_chunk.index.values.tolist()\n",
    "\n",
    "# Use our list of indices to reassign the big df index\n",
    "df.index =  index\n",
    "# Remove the 'level_0' and 'index' columns that resulted\n",
    "# from the above steps. This is not removing the actual index\n",
    "# of the df, just a data column CALLED index.\n",
    "df = df.loc[:,~df.columns.isin(['level_0','index'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few features to make sure our dataframe is as expected. We want to make sure the data import and aggregation steps worked well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for NaN entries (should not be any unless columns do not align), which can result from stitching together dfs with different values in their headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are any null values, then print names of columns containing\n",
    "# null values\n",
    "if df.isnull().any().any():\n",
    "    print(df.columns[df.isnull().any()])\n",
    "\n",
    "#in 'if' statement, false means no NaN entries True means NaN entries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all expected files were imported into final dataframe by comparing our sample names to the unique values in the Sample_ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sorted(df.Sample_ID.unique()) == sorted(ls_samples):\n",
    "    print(\"All expected filenames present in big df Sample_ID column.\")\n",
    "else:\n",
    "    compare_headers(['no samples'], df.Sample_ID.unique(), \"big df Sample_ID column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of header values that are not intensities. Can include items that aren't in a given header."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to save `not_intensities` list for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = os.path.join(metadata_dir,\"not_intensities.csv\")\n",
    "\n",
    "# If this file already exists, add only not_intensities items not already present in file\n",
    "if os.path.exists(fn):\n",
    "    print(\"'not_intensities.csv' already exists.\")\n",
    "    print(\"Reconciling file and Jupyter notebook lists.\")\n",
    "    # Open file as read-only, extract data\n",
    "    fh = open(fn, \"r\")\n",
    "    file_ni = fh.read().splitlines()\n",
    "    # Set difference to identify items not already in file\n",
    "    to_add = set(not_intensities) - set(file_ni)\n",
    "    # We want not_intensities to the a complete list\n",
    "    not_intensities = list(set(file_ni) | set(not_intensities))\n",
    "    fh.close()\n",
    "    # Open file for appending, writing new items\n",
    "    fh = open(fn, \"a\")\n",
    "    for item in to_add:\n",
    "        fh.write(item +\"\\n\")\n",
    "    fh.close()\n",
    "    \n",
    "# The file does not yet exist\n",
    "else:\n",
    "    print(\"Could not find \" + fn + \". Creating now.\")\n",
    "    # Open file for writing (will over-write exisiting file),\n",
    "    # write all items\n",
    "    fh = open(fn, \"w\")\n",
    "    for item in not_intensities:\n",
    "        fh.write(item + \"\\n\")\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unwanted columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are dropping a number of columns that we are totally uninterested in. For example, in the current workflow of QI Tissue, we can either export all columns (all markers in all cell components--cell, nucleus, cytoplasm) or individually check each and every one we want. It is faster and easier for the user, and maybe less error-prone, to export all columns and then drop those we are unintersted in here. Not every marker is expected to express in every location; this is why we might drop certain columns. Likewise, we may only be intersted in Average intensity in some features and Maximum intensity in others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For development purposes, we kept all marker columns in the Cell and that were Intensity Averages.\n",
    "# So the columns we want to keep:\n",
    "# not_intensities, and any intensity column that contains 'Intensity_Average'\n",
    "# We will be listing those columns we want to keep. Alternatively, you could name the columns you want to drop,\n",
    "# or a mixture of both tactics.\n",
    "\n",
    "# To get the 'Intensity_Average' columns, we use list comprehension:\n",
    "# first get a list of all df columns not in 'not_intensities', aka, \n",
    "# those that ARE intensities, 'x for x in df....'\n",
    "# Then, we only include them if they contain 'Intensity_Average',\n",
    "# \"...if 'Intensity_Average' in x\"\n",
    "\n",
    "## Explain how to add more, beyond Cell_Intensity_Average, etc.\n",
    "\n",
    "to_keep = not_intensities \\\n",
    "    + [x for x in df.columns.values[~df.columns.isin(not_intensities)] if 'Cell_Intensity_Average' in x]\n",
    "\n",
    "# If there are more columns we want to keep, we could include them by\n",
    "# adding them to our 'to_keep' list\n",
    "# to_keep.append(another_column)\n",
    "# NOTE - do NOT reassign this to to_keep (to_keep = to_keep.append(item)),\n",
    "# since the return value is None, for some reason. So you would be saying:\n",
    "# to_keep = to_keep.append(item)\n",
    "# to_keep = None\n",
    "# to_keep --> would display 'None'\n",
    "# to _keep = to_keep + [list, of, columns]\n",
    "# here, you DO ressign (list = list + other_list)\n",
    "\n",
    "# In order to extract only the columns we want from our big df, \n",
    "# we need to only ask for those that are IN the df.\n",
    "# Our to_keep list contains items that might not be in our df headers!\n",
    "# These items are from our not_intensities list. So let's ask for only those items\n",
    "# from to_keep that are actually found in our df\n",
    "df = df[[x for x in to_keep if x in df.columns.values]]\n",
    "\n",
    "\n",
    "# What if we want to drop certain markers by name?\n",
    "# Drop specific markers\n",
    "#df = df.drop(columns = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at column names to make sure they are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleus size analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only cells where nucleus_size is [0, 500]\n",
    "make_distr_plot_per_sample(\n",
    "    title = \"Initial dataframe nucleus sizes - 500 cutoff\",\n",
    "    location = output_images_dir, dfs = [df], \n",
    "    df_names = [\"Initial dataframe\"], colors = [\"blue\"], \n",
    "    x_label = \"Nucleus Size\", \n",
    "    legend = False, xlims = [0,500], markers = ['Nucleus_Size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only cells where nucleus_size is [0, 100]\n",
    "make_distr_plot_per_sample(title = \"Initial dataframe nucleus sizes to 100\",\n",
    "                           location = output_images_dir, dfs = [df], \n",
    "                           df_names = [\"Initial dataframe\"], colors = [\"blue\"], \n",
    "                           x_label = \"Nucleus Size\", \n",
    "                           legend = False, xlims = [0,100], markers = ['Nucleus_Size'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Peak analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find valleys between peaks in nucleus size data - unfinished, but left here in case it aids future development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfinished, but could consider using the following function\n",
    "m = signal.find_peaks(df[\"Nucleus_Size\"], prominence = 10, threshold = 20)\n",
    "m[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get quantiles (5th, 50th, 95th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [0.05,0.50,0.95] # list of nucleus size percentiles to extract \n",
    "# Extract quantiles\n",
    "nuc_sizes = pd.DataFrame(df[\"Nucleus_Size\"].quantile(q=qs))\n",
    "nuc_sizes['quantiles'] = nuc_sizes.index\n",
    "nuc_sizes = nuc_sizes.reset_index().drop(columns = ['index'])\n",
    "\n",
    "# Display df\n",
    "nuc_sizes\n",
    "## Save these data to file\n",
    "filename = \"nuc_quantile_sizes.csv\"\n",
    "filename = os.path.join(output_data_dir,filename)\n",
    "nuc_sizes.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nucleus size and other feature scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter plot â€“ to be most informative, ideally this would be cell size vs nucleus size, where color = nucleus roundness. Not all data used to develop workflow had all necessary features, so the actual data plotted below may not be terribly useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set string variables\n",
    "title = \"Nucleus size by cell size for initial dataframe\"\n",
    "x_label = \"Cell Size\"\n",
    "y_label = \"Nucleus Size\" # cell size - weewin data only has Nuc size!\n",
    "\n",
    "# Create figure\n",
    "fig = px.scatter(df, x=\"Cell_Size\", y=\"Nucleus_Size\",\n",
    "                 color='Nucleus_Roundness')\n",
    "\n",
    "#  Update layout for the aesthetic parameters we want\n",
    "fig.update_layout(title_text=title, font=dict(size=18), \n",
    "        plot_bgcolor = 'white', showlegend = True )\n",
    "# Adjust opacity\n",
    "fig.update_traces(opacity=0.6)\n",
    "# Adjust x-axis parameters\n",
    "fig.update_xaxes(title_text = x_label, showline=True, linewidth=2, linecolor='black', \n",
    "        tickfont=dict(size=18))\n",
    "    # Adjust y-axis parameters\n",
    "fig.update_yaxes(title_text = y_label, showline=True, linewidth=2, linecolor='black',\n",
    "        tickfont=dict(size=18))\n",
    "\n",
    "# Display plot\n",
    "#plot(fig)\n",
    "filename = os.path.join(output_images_dir, title.replace(\" \",\"_\") + \".png\")\n",
    "fig.write_image(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete columns as necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move forward with only the columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns containing \"DAPI\"\n",
    "# use list comprehension to extract only column headers\n",
    "# that do not contain the string \"DAPI\"\n",
    "df = df[[x for x in df.columns.values if 'DAPI' not in x]]\n",
    "\n",
    "print(\"Columns are now...\")\n",
    "print([c for c in df.columns.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lists of full names and shortened names to use in plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a list of shortened marker intensity column header values for use in plotting. For example 'pATR_Cell_Intensity_Average' would display as 'pATR' for readability. In the case of more than one column present for a given marker, e.g., the inclusion of 'pATR_Nucleus_Cell_Intensity_Average', the pltoted labels would be 'pATR_Cell' and 'pATR_Cell'. We want to create dictionaries of both full to short names and short to full names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_to_short_names, short_to_full_names =  \\\n",
    "    shorten_feature_names(df.columns.values[~df.columns.isin(not_intensities)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this data to a metadata file. These devices will be used throughout the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(metadata_dir, \"full_to_short_column_names.csv\")\n",
    "fh = open(filename, \"w\")\n",
    "fh.write(\"full_name,short_name\\n\")\n",
    "for k,v in full_to_short_names.items():\n",
    "    fh.write(k + \",\" + v + \"\\n\")\n",
    "    \n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(metadata_dir, \"short_to_full_column_names.csv\")\n",
    "fh = open(filename, \"w\")\n",
    "fh.write(\"short_name,full_name\\n\")\n",
    "for k,v in short_to_full_names.items():\n",
    "    fh.write(k + \",\" + v + \"\\n\")\n",
    "    \n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print contents to screen if the user wants\n",
    "\n",
    "#for key, value in full_to_short_names.items():\n",
    "#    print(key + \": \" + value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import exposure time metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we want to end up with a data structure that incorporates metadata on each intensity marker column used in our big dataframe in an easy-to-use format. This is going to include the full name of the intensity marker columns in the big data frame, the corresponding round and channel, the target protein (e.g., CD45), and the segmentation localization information (cell, cytoplasm, nucleus)... We can use this data structure to assign unique colors to all channels and rounds, for example, for use in later visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we expect this exposure time metadata file to have four columns (more are accepted). These are as follows:\n",
    "\n",
    "- Round: The round in which the marker was assess. Should be in form 'r#'\n",
    "- Target: The target/marker used. This should be a string whose contents match in the imported segmentation data files. The capitalization does not need to be consistent. These values should be unique in this file, without duplicates.\n",
    "- Exp: The exposre time for this marker for this channel, in milliseconds. Not currently used in workflow.\n",
    "- Channel: THe channel in which the marker was assessed. Should be in form 'c#'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Exposure_Time.csv\"\n",
    "#filename = \"Exposure_Time_full.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "\n",
    "exp_df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify file imported correctly\n",
    "\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, exp_df.shape[0] + 1)\n",
    "    print(\"Ran file size verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Headers\n",
    "print(\"Assessing whether column headers are as expected.\")\n",
    "expected_headers =['Round','Target','Exp','Channel']\n",
    "compare_headers(expected_headers, exp_df.columns.values, \"Imported metadata file\")\n",
    "\n",
    "# Missingness\n",
    "if exp_df.isnull().any().any():\n",
    "    print(\"\\nexp_df has null value(s) in row(s):\")\n",
    "    print(exp_df[exp_df.isna().any(axis=1)])\n",
    "else:\n",
    "    print(\"No null values detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure that there are not duplicate values in the Target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(exp_df['Target']) > len(exp_df['Target'].unique()):\n",
    "    print(\"One or more non-unique Target values in exp_df. Currently not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df.sort_values(by = ['Target']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lowercase version of target\n",
    "exp_df['target_lower'] = exp_df['Target'].str.lower()\n",
    "exp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe that contains marker intensity columns in our df that aren't in `not_intensities`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = pd.DataFrame({'full_column':df.columns.values[~df.columns.isin(not_intensities)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the marker information from the `full_column`, which corresponds to full column in big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions (regex) to isolate the part of the field that\n",
    "# begins (^) with an alphanumeric value (W), and ends with an underscore (_)\n",
    "# '$' is end of line\n",
    "intensities['marker'] = intensities['full_column'].str.extract(r'([^\\W_]+)')\n",
    "# convert to lowercase\n",
    "intensities['marker_lower'] = intensities['marker'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the intensities df to exclude any column pertaining to DAPI\n",
    "intensities = intensities.loc[intensities['marker_lower'] != 'dapi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now merge the `intensities` and `exp_df` together to create `metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.merge(exp_df, intensities, how = 'left',\n",
    "                   left_on = 'target_lower',right_on = 'marker_lower')\n",
    "metadata = metadata.drop(columns = ['marker_lower'])\n",
    "\n",
    "# Target is the capitalization from the Exposure_Time.csv\n",
    "# target_lower is Target in all caps\n",
    "# marker is the extracted first component of the full column in segmentation data, with corresponding capitalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a column to signify marker target location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a lambda to determine segmented location of intensity marker column and update metadata accordingly\n",
    "# This function determines what the location of the marker is in the cell\n",
    "# It looks for 'cytoplasm', 'cell',' or 'nucleus' string inside the \n",
    "# 'full_column' column of a given row, and returns the identifyied\n",
    "# area of 'unknown' if none of them\n",
    "def add_metadata_location(row):\n",
    "    fc = row['full_column'].lower()\n",
    "    if 'cytoplasm' in fc and 'cell' not in fc and 'nucleus' not in fc:\n",
    "        return 'cytoplasm'\n",
    "    elif 'cell' in fc and 'cytoplasm' not in fc and 'nucleus' not in fc:\n",
    "        return 'cell'\n",
    "    elif 'nucleus' in fc and 'cell' not in fc and 'cytoplasm' not in fc:\n",
    "        return 'nulceus'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# apply the function\n",
    "metadata['location'] = metadata.apply(\n",
    "    lambda row: add_metadata_location(row), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A peek at our `metadata` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this data structure to the metadata folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't want to add color in because that's better off treating color the same for round, channel, and sample\n",
    "filename = \"marker_intensity_metadata.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "metadata.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import sample metadata if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"ROI_Map.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "sample_metadata = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify file imported correctly\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, sample_metadata.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Headers\n",
    "print(\"Assessing whether column headers are as expected.\")\n",
    "expected_headers =['Sample_ID', 'ROI_slide','ROI_index', 'TMA_Core', 'TMA_row',\n",
    "                   'TMA_column', 'tissue_long', 'tissue_short', 'Replicate', 'Type']\n",
    "compare_headers(expected_headers, sample_metadata.columns.values, \"Imported metadata file\")\n",
    "\n",
    "# Missingness\n",
    "if exp_df.isnull().any().any():\n",
    "    print(\"\\nexp_df has null value(s) in row(s):\")\n",
    "    print(sample_metadta[sample_metadata.isna().any(axis=1)])\n",
    "else:\n",
    "    print(\"No null values detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, `sample_metadata` does not need to be merged with any other df and then saved again"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sample_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish colors to use throughout workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channel colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel colors - want colors that are categorical, since Channel is a non-ordered category (yes, they are numbered, but arbitrarily). A categorical color palette will have dissimilar colors. However, it we will typically use a prescribed set of channel colors that are consistent throughout experiments: c2 = green, c3 = orange, c4 = red, c5 = turquoise. The more automated channel color generation will be left below for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get those unique colors\n",
    "if len(metadata.Channel.unique()) > 10:\n",
    "    print(\"WARNING: There are more unique channel values than \\\n",
    "    there are colors to choose from. Select different palette, e.g., \\\n",
    "    continuous palette 'husl'.\")\n",
    "channel_color_values = sb.color_palette(\"colorblind\",n_colors = len(metadata.Channel.unique()))#'HLS'\n",
    "# chose 'colorblind' because it is categorical and we're unlikely to have > 10\n",
    "\n",
    "print(\"Unique channels are:\", metadata.Channel.unique())\n",
    "# Display those unique colors\n",
    "sb.palplot(sb.color_palette(channel_color_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_color_dict = dict(zip(metadata.Channel.unique(), channel_color_values))\n",
    "\n",
    "channel_color_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose our channel colors instead. We can use the function `matplotlib.colors.to_rbg(c)`, where `c` is a word color name, to convert to the (r, g, b) tuple needed for the workflow. At the top of the script, we imported `matplotlib.colors` as `mplc`, so we can save time and type out simply `mplc.to_rgb(c)` shorthand when using this function. Note that if you use any of the xkcd color survey colors (https://xkcd.com/color/rgb/), you will need to call these specify these as 'xkcd:colorname'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will demonstrate a couple of different ways of doing changing the colors we generated above, so the user can expand on the examples as necessary. We are holding all of our color information in several instances of a data structure called a dictionary. https://docs.python.org/3/library/stdtypes.html#typesmapping\n",
    "\n",
    "Dictionaries are a way to store an unordered collection of items where each is composed of a key-value mapped pair. In the case of this workflow, each color dictionary has a string identifying the specific thing to be colored, e.g., 'c2', 'TMA', 'cluster1', or 'r5', and the corresponding value is a three-float tuple (r, g, b) that is the color of that thing. With dictionaries, we can remove an key-value pair, add a new key-value pair, or overwrite an existing key-value pair whenever we want. Keys can be many things, but often you will see them as a string. Values can be strings, lists, other dictionaries (as seen below for the heatmaps), etc. Nested dictionaries can be complicated to intuit, but they can be a good way to associate a bunch of information together easily, coding-wise. Keys are not ordered within a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a new color for a channel, overwrite/replace the original channel color in the dictionary\n",
    "\n",
    "c2_color = \"green\"\n",
    "c2_color = mplc.to_rgb(\"green\")\n",
    "print(\"Our new color in rbg form is \" + str(c2_color) + \".\")\n",
    "\n",
    "print(\"Before replacement, c2 in the dictionary is: \" + str(channel_color_dict['c2']))\n",
    "\n",
    "# Replace value\n",
    "channel_color_dict['c2'] = c2_color\n",
    "print(\"After replacement, c2 in the dictionary is: \" + str(channel_color_dict['c2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how you delete an item from a dictionary\n",
    "\n",
    "print(\"Keys in the channel color dictionary are: \" + str(channel_color_dict.keys()))\n",
    "\n",
    "# If we try to remove an existing key, we will get an error\n",
    "if 'c2' in channel_color_dict.keys():\n",
    "    print(\"'c2' is in the dictionary. Removing now.\")\n",
    "    channel_color_dict.pop('c2')\n",
    "    \n",
    "print(\"Keys in the channel color dictionary are: \" + str(channel_color_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add in a new item\n",
    "print(\"Keys in the channel color dictionary are: \" + str(channel_color_dict.keys()))\n",
    "print(\"Adding in 'c2'...\")\n",
    "channel_color_dict['c2'] = c2_color\n",
    "print(\"Keys in the channel color dictionary are: \" + str(channel_color_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's finish the dictionary now\n",
    "\n",
    "channel_color_dict['c2'] = mplc.to_rgb('green')\n",
    "channel_color_dict['c3'] = mplc.to_rgb('orange')\n",
    "channel_color_dict['c4'] = mplc.to_rgb('red')\n",
    "channel_color_dict['c5'] = mplc.to_rgb('turquoise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## And display the colors so we can see them\n",
    "\n",
    "# Instead of querying the dictionary to get each of our colors, THEN putting those colors in a list,\n",
    "# THEN feeding that list into the palplot/color_palette code as above, I will condense these steps\n",
    "# together. Here we are accessing each (r,g,b) color value in the dictionary using the key.\n",
    "print(['c2','c3','c4','c5'])\n",
    "sb.palplot(sb.color_palette(\n",
    "    [channel_color_dict['c2'],channel_color_dict['c3'],channel_color_dict['c4'],channel_color_dict['c5']]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Round colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round colors - want colors that are sequential, since Round is an ordered category. We can still generate colors that are easy to distinguish. Also, many of the categorical palettes cap at at about 10 or so unique colors, and repeat from there. We do not want any repeats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_color_values = sb.cubehelix_palette(\n",
    "    len(metadata.Round.unique()), start=1, rot= -0.75, dark=0.19, light=.85, reverse=True)\n",
    "#round_color_values = sb.color_palette(\"cubehelix\",n_colors = len(metadata.Round.unique()))\n",
    "# chose 'cubehelix' because it is sequential, and round is a continuous process\n",
    "# each color value is a tuple of three values: (R, G, B)\n",
    "print(metadata.Round.unique())\n",
    "\n",
    "sb.palplot(sb.color_palette(round_color_values))\n",
    "\n",
    "## TO-DO: write what these parameters mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_color_dict = dict(zip(metadata.Round.unique(), round_color_values))\n",
    "\n",
    "for k,v in round_color_dict.items():\n",
    "    round_color_dict[k] = np.float64(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample colors - want colors that are neither sequential nor categorical. Categorical would be ideal if we could generate an arbitrary number of colors, but I do not think that we can. Hense, we will choose `n` colors from a continuous palette. First we will generate the right number of colors. Later, we will assign TMA samples to gray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get those unique colors\n",
    "color_values = sb.color_palette(\"husl\",n_colors = len(ls_samples))#'HLS'\n",
    "# each color value is a tuple of three values: (R, G, B)\n",
    "\n",
    "# Display those unique colors\n",
    "sb.palplot(sb.color_palette(color_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate enough gray shades for all TMA samples in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all TMA samples\n",
    "# by looking for substring 'TMA' in all unique Sample_ID values\n",
    "TMA_samples = [s for s in df.Sample_ID.unique() if 'TMA' in s]\n",
    "\n",
    "# Now make a list of unique gray shades,\n",
    "# whose length equals the length of the list above\n",
    "TMA_color_values = sb.color_palette(n_colors = len(TMA_samples),palette = \"gray\")\n",
    "\n",
    "# Show the gray color(s) to the user\n",
    "sb.palplot(sb.color_palette(TMA_color_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will create a dictionary to hold this information\n",
    "# Here we are mapping the unique Sample_ID values in df\n",
    "# (note that sorted() ensures they are in alphabetical\n",
    "# order) with the color_values list we derived above.\n",
    "# This list does NOT have our TMA gray(s) in it.\n",
    "# After we associate the two groups of items together\n",
    "# with zip, we turn it into a dictonary: key = Sample_ID,\n",
    "# value = color for that Sample_ID\n",
    "sample_color_dict = dict(zip(\n",
    "    sorted(df.Sample_ID.unique()), color_values\n",
    "            ))\n",
    "\n",
    "# Edit our dictioanry\n",
    "# Replace all TMA samples' colors with gray by\n",
    "# iterating through all keys in sorted order\n",
    "# and replacing the color with a gray one. We are\n",
    "# moving through our list of gray colors using our\n",
    "# index 'i', so that each TMA gets a different gray.\n",
    "i = 0\n",
    "for key in sorted(sample_color_dict.keys()):\n",
    "    if 'TMA' in key:\n",
    "        sample_color_dict[key] = TMA_color_values[i]\n",
    "        i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_color_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the (r,g,b) values of the colors above. Any TMA sample should have r ~= g ~= b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Our samples and corresponding colors are:\")\n",
    "print([key for key in sorted(sample_color_dict.keys())])\n",
    "sb.palplot(sb.color_palette([sample_color_dict[key] for key in sorted(sample_color_dict.keys())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save color information (mapping and legend) to metadata directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the metadata again...\n",
    "metadata.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add in the color information in both RGB (range 0-1) and hex values, for use in visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['round_color'] = metadata.apply(lambda row: round_color_dict[row['Round']], axis = 1)\n",
    "metadata['channel_color'] = metadata.apply(lambda row: channel_color_dict[row['Channel']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a dictionary cd, a column_name string\n",
    "# and returs a dataframe. This df has the information that was\n",
    "# in the dictionary--'rgb' is the (fl, fl, fl) tuple corresponding\n",
    "# to the color names given as the cd keys, an 'hex' is the corresponding\n",
    "# hexademical value.\n",
    "def color_dict_to_df(cd, column_name):\n",
    "    df = pd.DataFrame.from_dict(cd, orient = 'index')\n",
    "    df['rgb'] = df.apply(lambda row: (np.float64(row[0]), np.float(row[1]), np.float64(row[2])), axis = 1)\n",
    "    df = df.drop(columns = [0,1,2])\n",
    "    df['hex'] = df.apply(lambda row: mplc.to_hex(row['rgb']), axis = 1)\n",
    "    df[column_name] = df.index\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "color_df = color_dict_to_df(sample_color_dict, \"Sample_ID\")\n",
    "color_df.head()\n",
    "\n",
    "# Save to file in metadatadirectory\n",
    "filename = \"sample_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "color_df.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legend of sample info only\n",
    "\n",
    "g  = plt.figure(figsize = (1,1)).add_subplot(111)\n",
    "g.axis('off')\n",
    "handles = []\n",
    "# To change the order of items on the legend, do\n",
    "# for item in [item1, item2, item3]:\n",
    "for item in sorted(sample_color_dict.keys()):\n",
    "        h = g.bar(0,0, color = sample_color_dict[item],\n",
    "                  label = item, linewidth =0)\n",
    "        handles.append(h)\n",
    "first_legend = plt.legend(handles=handles, loc='upper right', title = 'Sample'),\n",
    "                            # bbox_to_anchor=(10,10), \n",
    "                             #       bbox_transform=plt.gcf().transFigure)\n",
    "\n",
    "# Save the legend to a file\n",
    "filename = \"Sample_legend.png\"\n",
    "filename = os.path.join(metadata_images_dir, filename)\n",
    "plt.savefig(filename, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "color_df = color_dict_to_df(channel_color_dict, \"Channel\")\n",
    "color_df.head()\n",
    "\n",
    "# Save to file in metadatadirectory\n",
    "filename = \"channel_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "color_df.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legend of channel info only\n",
    "\n",
    "g  = plt.figure(figsize = (1,1)).add_subplot(111)\n",
    "g.axis('off')\n",
    "handles = []\n",
    "# To change the order of items on the legend, do\n",
    "# for item in [item1, item2, item3]:\n",
    "for item in sorted(channel_color_dict.keys()):\n",
    "        h = g.bar(0,0, color = channel_color_dict[item],\n",
    "                  label = item, linewidth =0)\n",
    "        handles.append(h)\n",
    "first_legend = plt.legend(handles=handles, loc='upper right', title = 'Channel'),\n",
    "                            # bbox_to_anchor=(10,10), \n",
    "                             #       bbox_transform=plt.gcf().transFigure)\n",
    "\n",
    "# Save the legend to a file\n",
    "filename = \"Channel_legend.png\"\n",
    "filename = os.path.join(metadata_images_dir, filename)\n",
    "plt.savefig(filename, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "color_df = color_dict_to_df(round_color_dict, \"Round\")\n",
    "color_df.head()\n",
    "\n",
    "# Save to file in metadatadirectory\n",
    "filename = \"round_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "color_df.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legend of round info only\n",
    "\n",
    "round_legend  = plt.figure(figsize = (1,1)).add_subplot(111)\n",
    "round_legend.axis('off')\n",
    "handles = []\n",
    "# To change the order of items on the legend, do\n",
    "# for item in [item1, item2, item3]:\n",
    "for item in round_color_dict.keys():\n",
    "        h = round_legend.bar(0,0, color = round_color_dict[item],\n",
    "                  label = item, linewidth =0)\n",
    "        handles.append(h)\n",
    "first_legend = plt.legend(handles=handles, loc='upper right', title = 'Round'),\n",
    "                            # bbox_to_anchor=(10,10), \n",
    "                             #       bbox_transform=plt.gcf().transFigure)\n",
    "\n",
    "# Save the legend to a file\n",
    "filename = \"Round_legend.png\"\n",
    "filename = os.path.join(metadata_images_dir, filename)\n",
    "plt.savefig(filename, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplot of nucleus size by nucleus roundness, colored by sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was not working on my computer, probably due to the size of the data. Let's run this chunk using just a subset of the data. Here, we will want the subset to maintain the same proportion of cells for each Sample_ID as we had in the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_row_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = create_subset(df, 'Sample_ID', subset_row_count, 'original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lines for each sample ID are in our subset df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the proportions of cells in the original and subset dfs compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sample_ID'].value_counts().sort_index()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()/subset_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By sample ID only\n",
    "\n",
    "# initiate figure\n",
    "fig = go.Figure()\n",
    "title = 'Nucleus size by nucleus roundess by Sample ID'\n",
    "\n",
    "# plot each trace separately\n",
    "for sample in ls_samples:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x = subset_df.loc[subset_df['Sample_ID']==sample,'Nucleus_Roundness'],\n",
    "        y = subset_df.loc[subset_df['Sample_ID']==sample,'Nucleus_Size'],\n",
    "        mode = 'markers',\n",
    "        name = sample,\n",
    "        marker=dict(\n",
    "            color='rgb' + str(sample_color_dict[sample])),\n",
    "            showlegend = True\n",
    "        \n",
    "    ))\n",
    "    \n",
    "\n",
    "# Update figure for aesthetic details\n",
    "fig.update_layout(title = title, plot_bgcolor = 'white')\n",
    "fig.update_xaxes(title_text = \"Nucleus roundness\", linecolor = 'black')\n",
    "fig.update_yaxes(title_text = \"Nucleus size\", linecolor = 'black')\n",
    "\n",
    "# Output\n",
    "#plot(fig) # plot generates in new Chrome tab\n",
    "# Write to file\n",
    "filename = os.path.join(output_images_dir, title.replace(\" \",\"_\") + \".png\")\n",
    "fig.write_image(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only be plotting ~10k cells in the interest of time/computing resources. We want these 10k lines in our original df to be sampled randomly, without replacement, with the caveat that the proportions of all samples in the data are equal to each other (unless a particular sample does not have enough corresponding lines for the desired final df size). If the size of the dataframe is > 10k rows, then we will proceed with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_row_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = create_subset(df, 'Sample_ID', subset_row_count, 'equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lines for each sample ID are in our subset df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the proportions of cells in the original and subset dfs compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sample_ID'].value_counts().sort_index()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()/subset_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data structures to map colors to columns and rows..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the row colors, we essentially just need to map the information in a given feature to the colors that correspond to that value in the right color dictionary. For example, it might be sample_3, sample_3, sample_4, , so we need the row colors to be (1, 1, 1), (1, 1, 1), (0, 0.25, 0.6). These are the initialy colors--if we are clustering rows or columns, the labels will still match the data with which they're associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sample_colors = subset_df.Sample_ID.map(sample_color_dict)\n",
    "\n",
    "row_sample_colors[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For column rows, matching up the information in each column with the appropriate color is more difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we want to translate marker columns to their corresponding channel information,\n",
    "# and then match that up with the right color, as with row columns\n",
    "\n",
    "# First, we merge the (L) non-intensity column values, transformed into a dataframe,\n",
    "# with the metadata df (R), matching on the \"0\" column present in the L,\n",
    "# which is the only column in there, with the \"full_column\" (aka df header name)\n",
    "# column in the R, only including all cases where there is a match and any unmatched\n",
    "# L cases ('both' [?] would be only cases where ther is is a match, and 'right' would\n",
    "# be cases with a match and any unmatched R columns).\n",
    "column_channel_colors = pd.merge(pd.DataFrame(pd.Series(\n",
    "    subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values)), \n",
    "                  metadata, how = 'left',\n",
    "         left_on = 0, right_on = 'full_column'\n",
    "                                # From that resulting df, extract the '0' and 'Channel' objects,\n",
    "                                # then only 'Channel', then map to the right colors\n",
    "                                )[[0,'Channel']]['Channel'].map(channel_color_dict)\n",
    "\n",
    "# Set the index to be the names of the colors. There is only one column, and that is the corresponding\n",
    "# colors\n",
    "column_channel_colors.index = subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values\n",
    "\n",
    "column_channel_colors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we want to translate marker columns to their corresponding round information,\n",
    "# and then match that up with the right color, as with row columns\n",
    "\n",
    "# First, we merge the (L) non-intensity column values, transformed into a dataframe,\n",
    "# with the metadata df (R), matching on the \"0\" column present in the L,\n",
    "# which is the only column in there, with the \"full_column\" (aka df header name)\n",
    "# column in the R, only including all cases where there is a match and any unmatched\n",
    "# L cases ('both' [?] would be only cases where ther is is a match, and 'right' would\n",
    "# be cases with a match and any unmatched R columns).\n",
    "column_round_colors = pd.merge(pd.DataFrame(pd.Series(\n",
    "    subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values)), \n",
    "                  metadata, how = 'left',\n",
    "         left_on = 0, right_on = 'full_column'\n",
    "                              # From that resulting df, extract the '0' and 'Channel' objects,\n",
    "                                # then only 'Channel', then map to the right colors\n",
    "                              )[[0,'Round']]['Round'].map(round_color_dict)\n",
    "\n",
    "# Set the index to be the names of the colors. There is only one column, and that is the corresponding\n",
    "# colors\n",
    "column_round_colors.index = subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values\n",
    "\n",
    "column_round_colors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data structure to hold everything we need for row/column annotations\n",
    "# annotations is a dictionary\n",
    "## IMPORTANT - if you use 'annotations', it MUST have both 'rows' and 'cols'\n",
    "## objects inside. These can be empty lists, but they must be there!\n",
    "annotations = {}\n",
    "\n",
    "# create a data structure to hold everything we need for only row annotations\n",
    "# row_annotations is a list, where each item therein is a dictioary corresponding\n",
    "# to all of the data pertaining to that particular annotation\n",
    "# Adding each item (e.g., Sample, then Cluster), one at a time to ensure ordering\n",
    "# is as anticipated on figure\n",
    "row_annotations = []\n",
    "row_annotations.append({'label':'Sample','type':'row','mapping':row_sample_colors,'dict':sample_color_dict,\n",
    "                        'location':'center left','bbox_to_anchor':(0, 0.5)})\n",
    "# Add all row information into the annotations dictionary\n",
    "annotations['rows'] = row_annotations\n",
    "\n",
    "\n",
    "# Now we repeat the process for column annotations\n",
    "col_annotations = []\n",
    "col_annotations.append({'label':'Round','type':'column','mapping':column_round_colors,'dict':round_color_dict,\n",
    "                       'location':'upper right','bbox_to_anchor':(1,0.50)})\n",
    "\n",
    "col_annotations.append({'label':'Column','type':'column','mapping':column_channel_colors,'dict':channel_color_dict,\n",
    "                       'location':'upper right','bbox_to_anchor':(1,0.75)})\n",
    "annotations['cols'] = col_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually plot the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_function(\n",
    "    data = subset_df.loc[:,~subset_df.columns.isin(not_intensities)],\n",
    "    title = \"Initial dataframe\",\n",
    "    # define method, metric, and color map\n",
    "    method = 'ward', metric = 'euclidean',cmap = 'coolwarm',\n",
    "    # colorbar info (legend coloring of main plot) \n",
    "    cbar_kws = {'label':'Intens.'},\n",
    "    # xticklabels - want to have the nicknames instead of full names,\n",
    "    # so we translate from full to short names; we also only want to include\n",
    "    # non_intensity columns, to match the data we fed into under 'data'\n",
    "    xticklabels = [full_to_short_names[name] for name in \n",
    "                     subset_df.loc[:,\n",
    "                                 ~subset_df.columns.isin(not_intensities)].columns.values],\n",
    "    # where to save the df\n",
    "    save_loc = output_images_dir,\n",
    "    # how to cluster on rows and columns\n",
    "    row_cluster = True, col_cluster = True,\n",
    "    # provide the dictionary of row and column coloring information\n",
    "    # and legend information, as established above.\n",
    "    annotations = annotations\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plot of count of all cells in all samples - no filtering yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts for each Sample_ID, sorted by Sample_ID\n",
    "counts = pd.DataFrame(df.Sample_ID.value_counts()).sort_index()\n",
    "\n",
    "# rename Sample_ID to counts\n",
    "counts = counts.rename(columns = {'Sample_ID':'counts'})\n",
    "# add Sample_ID back in, as what's currently the index\n",
    "counts['Sample_ID'] = counts.index\n",
    "# add 'color', which is derived from the row's Sample_ID fed into the right\n",
    "# color dictionary\n",
    "counts['color'] = counts.apply(lambda row: sample_color_dict[row['Sample_ID']], axis = 1)\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By sample ID only\n",
    "\n",
    "# establish figure\n",
    "fig = go.Figure()\n",
    "title = 'Initial Cell counts by Sample ID'\n",
    "\n",
    "# Changing the ordering of the bars is a easy as iterating through a list\n",
    "# with the samples in a different order! For example, this order below:\n",
    "#for sample in ['TMA', 'GZ7.2', 'GZ10.3', 'GZ7.1', 'GZ10.2', 'GZ10.1', 'GZ6']:\n",
    "for sample in ls_samples:\n",
    "    # add trace for each sample\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=counts.loc[counts['Sample_ID']==sample,'Sample_ID'], \n",
    "        y = counts.loc[counts['Sample_ID']==sample,'counts'],\n",
    "        text = counts.loc[counts['Sample_ID']==sample,'counts'], \n",
    "        textposition='outside',\n",
    "        marker=dict(\n",
    "            color='rgb' + str(sample_color_dict[sample])),\n",
    "            showlegend = False\n",
    "        \n",
    "    ))\n",
    "    \n",
    "# update aesthetic parameters\n",
    "fig.update_layout(title = title, plot_bgcolor = 'white')\n",
    "fig.update_xaxes(title_text = \"Sample ID\", linecolor = 'black')\n",
    "fig.update_yaxes(title_text = \"Cell count\", linecolor = 'black')\n",
    "\n",
    "# Display plot\n",
    "#plot(fig)\n",
    "filename = os.path.join(output_images_dir, title.replace(\" \",\"_\") + \".png\")\n",
    "fig.write_image(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you might save data for the PCA, if you'd like to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for PCA\n",
    "filename = \"weewin_PCA_test.csv\"\n",
    "filename = \"repro_PCA_test.csv\"\n",
    "filename = \"gz_PCA_test.csv\"\n",
    "df.to_csv(filename, index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any other rows or columns we want to before saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of example, I will operate on a copy of df, called df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST do df.copy()\n",
    "# 'df_copy = df' would essentially \n",
    "# give you two different names for the\n",
    "# SAME dataframe, so operating on one\n",
    "# would also operate on the other\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operate on entire rows or columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "my_cols = []\n",
    "df_copy = df_copy.drop(columns = my_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only specific columns (explained below)\n",
    "my_cols = []\n",
    "my_cols = df.columns.values\n",
    "df_copy = df_copy.loc[:,my_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operate on rows and columns using filtering criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only certain rows based off of criteria\n",
    "\n",
    "# use df.loc[] to filter\n",
    "# df.loc[rows,columns]\n",
    "# df.loc[:,certain_cols] --> keep all rows ':', only certain cols\n",
    "# df.loc[certain_rows,:] --> keep only certain row, all cols ':'\n",
    "\n",
    "# Say we only want certain values for Sample_ID\n",
    "print(df_copy.Sample_ID.unique())\n",
    "keep = ['TMA','GZ6']\n",
    "#keep = ['TMA1.1','TMA1.2','TMA1.3','TMA2.1','TMA2.2','TMA2.3']\n",
    "df_copy = df_copy.loc[df_copy['Sample_ID'].isin(keep),:]\n",
    "print(df_copy.Sample_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on multiple criteria\n",
    "# '&' or 'and'\n",
    "# '|' or 'or'\n",
    "# you MUST have parentheses around each logic expression!\n",
    "df_copy = df_copy.loc[\n",
    "    (df_copy['Sample_ID'].isin(['TMA1.1','TMA1.2','TMA1.3'])) \\\n",
    "    ## backslash above used to break line for readability, but tell Python to act like it's all one line\n",
    "        | (df_copy['Sample_ID'].isin(['TMA2.1','TMA2.2','TMA2.3'])) , :]\n",
    "print(df_copy.Sample_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows based off of certain criteria\n",
    "# note the negating tilde '~'!\n",
    "\n",
    "df_copy = df_copy.loc[\n",
    "    (~df_copy['Sample_ID'].isin(['TMA1.1','TMA1.2','TMA1.3'])) \\\n",
    "    ## backslash above used to break line for readability, but tell Python to act like it's all one line\n",
    "        & (~df_copy['Sample_ID'].isin(['TMA2.1','TMA2.2','TMA2.3'])),:]\n",
    "print(df_copy.Sample_ID.unique())\n",
    "\n",
    "## include example for cell types: cancer, stroma, immune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data by Sample_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of output file first\n",
    "for sample in ls_samples:\n",
    "    filename = os.path.join(output_data_dir,  sample + \"_\" + step_suffix + \".csv\")\n",
    "    if os.path.exists(filename):\n",
    "        print(\"File by name \"+filename+\" already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output files\n",
    "for sample in ls_samples:\n",
    "    df_save = df.loc[df['Sample_ID'] == sample,:]\n",
    "    filename = os.path.join(output_data_dir,  sample + \"_\" + step_suffix + \".csv\")\n",
    "    df_save.to_csv(filename, index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans Notebook\n",
    "\n",
    "By: Megan Grout (groutm2020@alumni.ohsu.edu)\n",
    "\n",
    "Adapted from code written by Dr. Marilyne Labrie and Nick Kendsersky\n",
    "\n",
    "\n",
    "Last updated: 20200527"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import external libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mplc\n",
    "\n",
    "\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \n",
    "import plotly.express as px\n",
    "init_notebook_mode(connected = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import functions written for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cycif_modules import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to change header names. Not encapsutated in `cycif_modules`, so that user can change on the fly as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may change for each experiment, so I have not sequestered\n",
    "# this code in the my_modules.py file\n",
    "\n",
    "# This function takes in a dataframe, changes the names\n",
    "# of the column in various ways, and returns the dataframe.\n",
    "# For best accuracy and generalizability, the code uses\n",
    "# regular expressions (regex) to find strings for replacement.\n",
    "def apply_header_changes(df):\n",
    "    # remove lowercase x at beginning of name\n",
    "    df.columns = df.columns.str.replace(\"^x\",\"\")\n",
    "    # remove space at beginning of name\n",
    "    df.columns = df.columns.str.replace(\"^ \",\"\")\n",
    "    # replace space with underscore\n",
    "    df.columns = df.columns.str.replace(\" \",\"_\")\n",
    "    # fix typos\n",
    "    #df.columns = df.columns.str.replace(\"typo\",\"correct_name\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory for project\n",
    "base_dir = ''\n",
    "\n",
    "\n",
    "# Set name for of project\n",
    "# for use in directory creation\n",
    "project_name = ''\n",
    "\n",
    "# Set string for current step, and for previous step\n",
    "# for use in file and direcotry naming\n",
    "step_suffix = 'kmeans'\n",
    "previous_step_suffix_long = \"_zscore\"\n",
    "\n",
    "# Initial input data directory\n",
    "input_data_dir = os.path.join(base_dir, project_name + previous_step_suffix_long)\n",
    "\n",
    "\n",
    "# KMeans directory\n",
    "output_data_dir = os.path.join(base_dir, project_name + \"_\" + step_suffix)\n",
    "\n",
    "# KMeans images subdirectory\n",
    "output_images_dir = os.path.join(output_data_dir,\"images\")\n",
    "\n",
    "# Metadata directories\n",
    "metadata_dir = os.path.join(base_dir, project_name + \"_metadata\")\n",
    "metadata_images_dir = os.path.join(metadata_dir,\"images\")\n",
    "\n",
    "# Create necessary directories for this step, if they don't already exist\n",
    "for d in [base_dir, input_data_dir, output_data_dir, output_images_dir, \n",
    "          metadata_dir, metadata_images_dir]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "# Change directory to location of input files        \n",
    "os.chdir(input_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of samples for use in this step of workflow. Do not include file extensions or steps labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comment for final workflow\n",
    "\n",
    "ls_samples = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all metadata we need from the QC/EDA chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"marker_intensity_metadata.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "metadata = pd.read_csv(filename)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, metadata.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "# Verify headers\n",
    "exp_cols = ['Round','Target','Channel','target_lower','full_column','marker','location']\n",
    "compare_headers(exp_cols, metadata.columns.values, \"Marker metadata file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some of dataframe - FYI\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### not_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"not_intensities.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "not_intensities = []\n",
    "with open(filename, 'r') as fh:\n",
    "    not_intensities = fh.read().strip().split(\"\\n\")\n",
    "    # take str, strip whitespace, split on new line character\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, not_intensities.shape[0])\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Print to console\n",
    "print(\"not_intensities = \")\n",
    "print(not_intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full_to_short_column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"full_to_short_column_names.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "full_to_short_names = df.set_index('full_name').T.to_dict('records')[0]\n",
    "\n",
    "# Print information\n",
    "print('full_to_short_names =')\n",
    "print(full_to_short_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short_to_full_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"short_to_full_column_names.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "short_to_full_names = df.set_index('short_name').T.to_dict('records')[0]\n",
    "\n",
    "# Print information\n",
    "print('short_to_full_names =')\n",
    "print(short_to_full_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sample_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "df = df.drop(columns = ['hex'])\n",
    "\n",
    "# our tuple of float values for rgb, (r, g, b) was read in \n",
    "# as a string '(r, g, b)'. We need to extract the r-, g-, and b-\n",
    "# substrings and convert them back into floats\n",
    "df['rgb'] = df.apply(lambda row: rgb_tuple_from_str(row['rgb']), axis = 1)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "sample_color_dict = df.set_index('Sample_ID').T.to_dict('rgb')[0]\n",
    "\n",
    "# Print information\n",
    "print('sample_color_dict =')\n",
    "print(sample_color_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"channel_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "df = df.drop(columns = ['hex'])\n",
    "\n",
    "# our tuple of float values for rgb, (r, g, b) was read in \n",
    "# as a string '(r, g, b)'. We need to extract the r-, g-, and b-\n",
    "# substrings and convert them back into floats\n",
    "df['rgb'] = df.apply(lambda row: rgb_tuple_from_str(row['rgb']), axis = 1)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "channel_color_dict = df.set_index('Channel').T.to_dict('rgb')[0]\n",
    "\n",
    "# Print information\n",
    "print('channel_color_dict =')\n",
    "print(channel_color_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"round_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "df = df.drop(columns = ['hex'])\n",
    "\n",
    "# our tuple of float values for rgb, (r, g, b) was read in \n",
    "# as a string '(r, g, b)'. We need to extract the r-, g-, and b-\n",
    "# substrings and convert them back into floats\n",
    "df['rgb'] = df.apply(lambda row: rgb_tuple_from_str(row['rgb']), axis = 1)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "round_color_dict = df.set_index('Round').T.to_dict('rgb')[0]\n",
    "\n",
    "# Print information\n",
    "print('round_color_dict =')\n",
    "print(round_color_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"celltype_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "\n",
    "# Check file exists\n",
    "if not os.path.exists(filename):\n",
    "    print(\"WARNING: Could not find desired file: \"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open, read in information\n",
    "df = pd.read_csv(filename, header = 0)\n",
    "df = df.drop(columns = ['hex'])\n",
    "\n",
    "# our tuple of float values for rgb, (r, g, b) was read in \n",
    "# as a string '(r, g, b)'. We need to extract the r-, g-, and b-\n",
    "# substrings and convert them back into floats\n",
    "df['rgb'] = df.apply(lambda row: rgb_tuple_from_str(row['rgb']), axis = 1)\n",
    "\n",
    "# Verify size\n",
    "# This part is wrapped in a try/except block because \n",
    "# it wasn't working on the PC workstation, but worked\n",
    "# on MG's personal PC laptop and department loaner MacBook\n",
    "try:\n",
    "    verify_line_no(filename, df.shape[0] + 1)\n",
    "    print(\"Ran file length verification.\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Turn into dictionary\n",
    "celltype_color_dict = df.set_index('cell_type').T.to_dict('rgb')[0]\n",
    "\n",
    "# Print information\n",
    "print('celltype_color_dict =')\n",
    "print(celltype_color_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt user for any files they would like excluded from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the first row of the file correpsonding to the first sample (index = 0)\n",
    "# in ls_samples\n",
    "\n",
    "# We do not need to specify a directory, since we earlier changed\n",
    "# the current working directory to be that containing these files\n",
    "filename = ls_samples[0] + previous_step_suffix_long + \".csv\"\n",
    "\n",
    "# Read in only the first line\n",
    "df = pd.read_csv(filename, index_col = 0, nrows = 1)\n",
    "\n",
    "# Apply the changes to the headers as specified in above funciton\n",
    "df = apply_header_changes(df)\n",
    "\n",
    "# Set variable to hold default header values\n",
    "expected_headers = df.columns.values\n",
    "\n",
    "print(\"df index name is currently\",df.index.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Used \" + ls_samples[0] + previous_step_suffix_long +\n",
    "      \".csv to determine the expected, corrected headers for all files.\")\n",
    "print(\"There headers are: \\n\" + \", \".join([h for h in expected_headers]) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dictionary to hold all individual sample data\n",
    "dfs = {}\n",
    "\n",
    "# iterate through each sample in our list of samples\n",
    "for sample in ls_samples:\n",
    "    # Check for existence of file\n",
    "    if not os.path.exists(sample+previous_step_suffix_long+\".csv\"):\n",
    "        print(\"File \" + sample+previous_step_suffix_long+\".csv\" +\n",
    "             \" does not exist. Removing from analysis...\")\n",
    "        # Remove from list if not found\n",
    "        ls_samples.remove(sample)\n",
    "        continue\n",
    "        \n",
    "    # open the file\n",
    "    # set the index to be the first (0-based indexing, so 0th)\n",
    "    # column in input file.\n",
    "    df = pd.read_csv(sample + previous_step_suffix_long + \".csv\", \n",
    "                     index_col = 0) #,  nrows = 500)\n",
    "    # use nrows to specify the number of rows you want\n",
    "    \n",
    "    # Check for empty df\n",
    "    # if so, don't continue trying to process df\n",
    "    if df.shape[0] == 0:\n",
    "        print('Zero content lines detected in ' + sample + ' file.'\n",
    "              'Removing from analysis...')\n",
    "        # Remove from list, so further steps won't be looking\n",
    "        # for data on this sample.\n",
    "        # Note that for lists, we do not need to re-assign\n",
    "        # the list when removing an item, i.e., we do not say\n",
    "        # 'ls_samples = ls_samples.remove(sample)', since this\n",
    "        # operation does not return anything.\n",
    "        ls_samples.remove(sample)\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    # Verify that the loaded df are the right length\n",
    "    # commenting out because this code did not work on all\n",
    "    # machines during testing (failed one PC, succeeded with\n",
    "    # one PC and one MacBook)\n",
    "    try:\n",
    "        verify_line_no(sample + \".csv\", df.shape[0] + 1) \n",
    "    except:\n",
    "        pass\n",
    "    # adding 1 because we expect the header was detected \n",
    "    # during file import and not counted towards length of df\n",
    "    \n",
    "     # Manipulations necessary for concatenation\n",
    "    df = apply_header_changes(df)\n",
    "    # sort them alphanetically\n",
    "    df = df[[x for x in sorted(df.columns.values)]]\n",
    "    \n",
    "    # Compare headers of new df against what is expected\n",
    "    compare_headers(expected_headers, df.columns.values, sample)\n",
    "\n",
    "    # For cases where we have samples called TMA1.1, TMA1.2, TMA1.3, etc.\n",
    "    # Using regular expressions (regex) to extract the characters in the\n",
    "    # sample name from TMA to the following digits, stopping at the period\n",
    "    #if 'ROI_index' in df.columns.values:\n",
    "    #    df['ROI_slide'] = re.findall(r'(TMA\\d+)',sample)[0]    \n",
    "    \n",
    "    # Add to dictonary of dfs \n",
    "    dfs[sample] = df\n",
    "    \n",
    "\n",
    "#Merge dfs into one big df\n",
    "df = pd.concat(dfs.values(), ignore_index=False , sort = False)\n",
    "# remove dfs from memory, since its big (relatively) and we\n",
    "# don't need a data struture of all samples' data separated\n",
    "# individually when we can extract information from the big\n",
    "# df using the Sample_ID column\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a few features to make sure our dataframe is as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for NaN entries (should not be any unless columns do not align), which can result from stitching together dfs with different values in their headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are any null values, then print names of columns containing\n",
    "# null values\n",
    "if df.isnull().any().any():\n",
    "    print(df.columns[df.isnull().any()])\n",
    "\n",
    "#in 'if' statement, false means no NaN entries True means NaN entries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that all expected files were imported into final dataframe by comparing our sample names to the unique values in the Sample_ID column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all expected files were imported into final dataframe\n",
    "\n",
    "if sorted(df.Sample_ID.unique()) == sorted(ls_samples):\n",
    "    print(\"All expected filenames present in big df Sample_ID column.\")\n",
    "else:\n",
    "    compare_headers(['no samples'], df.Sample_ID.unique(), \"big df Sample_ID column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will determine the best number of clusters, K, to use in our KMeans clustering. We may decide to operate on a random subset of our data (with Sample_ID proportions the same), in order to save time. Then, once the 'best' K is determined using the elbox method and chosen algorithm, we will perform KMeans clustering, using that K, on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the following references for more information and methods on finding the elbow/knee in our KMeans clustering:\n",
    "\n",
    "- https://github.com/arvkevi/kneed\n",
    "- https://raghavan.usc.edu//papers/kneedle-simplex11.pdf\n",
    "- https://github.com/arvkevi/kneed/blob/master/notebooks/decreasing_function_walkthrough.ipynb\n",
    "- https://www.scikit-yb.org/en/latest/api/cluster/elbow.html#elbow-method\n",
    "- https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find elbow and plot all at once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resource is a library that will take your data, find the 'best' K, and supply your data using this best 'K'. It can also be used to plot the performance of different K values. Since--as far as I can tell--it does not allow the user to tweak enough parameters in the K determination or the plotting, I have in 'raw' format in the code so that it will not run. However, since it makes the process so automated, it has the potential to be very useful for the user, and so I am leaving it in here."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# Generate synthetic dataset with 8 random clusters\n",
    "\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "viz = KElbowVisualizer(model, k=(1,20), timings = False,\n",
    "                             metric = 'distortion')\n",
    "\n",
    "viz.fit(df.loc[:,~df.columns.isin(not_intensities)])        # Fit the data to the visualizer\n",
    "viz.show()        # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this doesn't allow us to modify/stylize the plot the way we want, and doesn't lend itself well to a dashboard. We also cannot determine the metric used to calculated `cdist()` to caluclate distorition (Euclidean, sqeuclidean, etc.), just that the metric for the visualization *overall* is distortion. It looks like the value for every *K* is the `inertia_` object from our `KMeans()` model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above references to calculate the intertia and distortion scores manually. Here, we will use the `k_scores_` values from our `viz` object above to create a custom plot that matches the style of the rest of the workflow and can be edited as seen fit. The `k_scores_` values are essentially equivalent to the \"manual\" intertia calculatios, despite the fact the metric used will be `\"distortion\"`. But we can still use this to find our infection point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_row_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = create_subset(df, 'Sample_ID', subset_row_count, 'original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lines for each sample ID are in our subset df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the proportions of cells in the original and subset dfs compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sample_ID'].value_counts().sort_index()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model and calculate interias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans()\n",
    "viz = KElbowVisualizer(model, k=(1,20), timings = False,\n",
    "                             metric = 'distortion')\n",
    "\n",
    "viz.fit(subset_df.loc[:,subset_df.columns.isin(not_intensities)])        # Fit the data to the visualizer\n",
    "inertias = viz.k_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot inertia for each K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish figure\n",
    "fig = go.Figure()\n",
    "title = 'K-Means cluster count determination'\n",
    "\n",
    "# Add data to figure\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        # Plot inertias by K value\n",
    "        x=list(K), y=inertias,\n",
    "        # Show points and connecting lines\n",
    "        mode='lines+markers',\n",
    "        # Marker/lione aesthetics\n",
    "        marker=dict(\n",
    "            color='LightSkyBlue',\n",
    "            size=15,\n",
    "            line=dict(\n",
    "                color='MediumPurple',\n",
    "                width=2\n",
    "            ))))\n",
    "    \n",
    "# Update figure aesthetics    \n",
    "fig.update_layout(title = title, plot_bgcolor = 'white')\n",
    "\n",
    "# Update figure axes\n",
    "fig.update_xaxes(title_text = 'Number of clusters', linecolor = 'black')\n",
    "fig.update_yaxes(title_text = 'Inertia', linecolor = 'black', \n",
    "                 range = [0, max(inertias)+0.1*max(inertias)])\n",
    "# Figure output\n",
    "#plot(fig)\n",
    "filename = title.replace(\" \",\"_\") + \".png\"\n",
    "filename = os.path.join(output_images_dir, filename)\n",
    "fig.write_image(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find elbow in above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kn = KneeLocator(list(K), inertias, S=1.0, curve='convex', direction='decreasing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was determined to be the best 'knee' using the KneeLocator() function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_kn.knee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the same figure as above, but include a vertical line at the `best_kn.knee` K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish figure\n",
    "fig = go.Figure()\n",
    "title = 'K-Means cluster count determination with elbow'\n",
    "\n",
    "# Add data to figure\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        # Plot score by K value\n",
    "        x=list(K), y=inertias, \n",
    "        # We want points and connecting lines\n",
    "        mode='lines+markers',\n",
    "        # Marker/line parameters\n",
    "        marker=dict(\n",
    "            color='LightSkyBlue', size=15,\n",
    "            line=dict(\n",
    "                color='MediumPurple',width=2\n",
    "                        ))))\n",
    "# Add in vertial line   \n",
    "fig.add_shape(\n",
    "    # Line Vertical\n",
    "    go.layout.Shape(\n",
    "        type=\"line\",\n",
    "        xref = \"x\",\n",
    "        yref = \"y\",\n",
    "        x0=best_kn.knee,\n",
    "        y0=0,\n",
    "        x1=best_kn.knee,\n",
    "        # Determine the height of the line.\n",
    "        # Can distort plot if too long.\n",
    "        y1= max(inertias)+0.1*max(inertias),\n",
    "        # Line aesthetics\n",
    "        line=dict(\n",
    "            color=\"black\", width=2, dash = 'dot'\n",
    "        ),\n",
    "))   \n",
    "\n",
    "# Update figure aesthetics\n",
    "fig.update_layout(title = title, plot_bgcolor = 'white')\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(title_text = 'Number of clusters', linecolor = 'black')\n",
    "fig.update_yaxes(title_text = 'Inertia', linecolor = 'black',\n",
    "                 range = [0, max(inertias)+0.1*max(inertias)])\n",
    "\n",
    "# Plot output\n",
    "#plot(fig)\n",
    "fig.write_image(output_images_dir + \"/\" + title.replace(\" \",\"_\") + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform KMean clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using the K value determined by the `KneeLocator`. We are going to operate on the full `df`, not a subset, since to esrablish cluster IDs for each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = best_kn.knee\n",
    "#n_clusters = \n",
    "n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KMeans Clustering on mean intensities\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "cluster = KMeans(n_clusters = n_clusters, random_state = 20) #number of clusters can be set here \n",
    "\n",
    "# We are performing KMeans clustering on df using K (5) clusters\n",
    "cluster.fit_predict(df.loc[:,~df.columns.isin(not_intensities)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update `df` to include cluster information for each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column with cluster number \n",
    "# We don't want our cluster labels to start with '0'\n",
    "df['cluster'] = cluster.labels_ + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the full dataset with the clustering column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Kmeans_full_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create color dictionary for clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, we to choose colors that are categorical, since Cluster is actually a non-ordered category. However, since we could conceivably be working with > 10-11 clusters, we do not want a color palette that will just cycle back through the same limited colors, so we are going to take this continuous color palette and get the number of unique colors we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get those unique colors\n",
    "cluster_color_values = sb.color_palette(\"hls\",n_colors = len(df.cluster.unique()))#'HLS'\n",
    "\n",
    "print(sorted(df.cluster.unique()))\n",
    "# Display those unique colors\n",
    "sb.palplot(sb.color_palette(cluster_color_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_color_dict = dict(zip(sorted(df.cluster.unique()), cluster_color_values))\n",
    "cluster_color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here is an example of how you might specify the values yourself,\n",
    "# derived from the QC/EDA chapter. Note that in this case, our keys\n",
    "# are not strings (e.g., '1'), but are actually ints (integers, e.g., 1).\n",
    "\n",
    "#cluster_color_dict['1'] = mplc.to_rgb('xkcd:dark sky blue')\n",
    "#cluster_color_dict['2'] = mplc.to_rgb('xkcd:reddish orange')\n",
    "#cluster_color_dict['3'] = mplc.to_rgb('xkcd:jungle green')\n",
    "\n",
    "\n",
    "sb.palplot(sb.color_palette(\n",
    "    [cluster_color_dict[1],\n",
    "     cluster_color_dict[2],\n",
    "     cluster_color_dict[3],\n",
    "    cluster_color_dict[4],\n",
    "    cluster_color_dict[5]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save color information (mapping and legend) to metadata directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "cluster_color_df = color_dict_to_df(cluster_color_dict, \"cluster\")\n",
    "cluster_color_df.head()\n",
    "\n",
    "# Save to file in metadatadirectory\n",
    "filename = \"cluster_color_data.csv\"\n",
    "filename = os.path.join(metadata_dir, filename)\n",
    "cluster_color_df.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legend of cluster info only\n",
    "\n",
    "g  = plt.figure(figsize = (1,1)).add_subplot(111)\n",
    "g.axis('off')\n",
    "handles = []\n",
    "for item in sorted(cluster_color_dict.keys()):\n",
    "        h = g.bar(0,0, color = cluster_color_dict[item],\n",
    "                  label = item, linewidth =0)\n",
    "        handles.append(h)\n",
    "first_legend = plt.legend(handles=handles, loc='upper right', title = 'Cluster'),\n",
    "\n",
    "\n",
    "filename = \"Clustertype_legend.png\"\n",
    "filename = os.path.join(metadata_images_dir, filename)\n",
    "plt.savefig(filename, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I assume we have clustering data for the full df loaded in for this chapter, not just a subset of ~10k cells, regardless of how many rows of our df were used to determine the best K. If we *are* working with a subsetted df at this point, then the below method to get a subset appropriate for the heatmap will still work fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only be plotting ~10k cells in the interest of time/computing resources. We want these 10k lines in our original df to be sampled randomly, without replacement, with the caveat that the proportions of all samples in the data remains the same in this subset. If the size of the dataframe is > 10k rows, then we will proceed with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_row_count = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = create_subset(df, 'Sample_ID', subset_row_count, 'equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lines for each sample ID are in our subset df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df['Sample_ID'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the proportions of cells in the original and subset dfs compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sample_ID'].value_counts().sort_index()/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Unlike other heatmaps in the workflow, this one will not use row clustering. We want to arrange the rows first by cluster number and then by Sample_ID.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = subset_df.sort_values(by = ['cluster','Sample_ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data structures to map colors to columns and rows..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the row colors, we essentially just need to map the information in a given feature to the colors that correspond to that value in the right color dictionary. For example, it might be sample_3, sample_3, sample_4, , so we need the row colors to be (1, 1, 1), (1, 1, 1), (0, 0.25, 0.6). These are the initialy colors--if we are clustering rows or columns, the labels will still match the data with which they're associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row_colors = subset_df.Sample_ID.map(sample_color_dict)\n",
    "\n",
    "sample_row_colors[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_celltype_colors = subset_df.cell_type.map(celltype_color_dict)\n",
    "\n",
    "row_celltype_colors[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_cluster_colors = subset_df.cluster.map(cluster_color_dict)\n",
    "\n",
    "row_cluster_colors[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For column rows, matching up the information in each column with the appropriate color is more difficult. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we want to translate marker columns to their corresponding channel information,\n",
    "# and then match that up with the right color, as with row columns\n",
    "\n",
    "# First, we merge the (L) non-intensity column values, transformed into a dataframe,\n",
    "# with the metadata df (R), matching on the \"0\" column present in the L,\n",
    "# which is the only column in there, with the \"full_column\" (aka df header name)\n",
    "# column in the R, only including all cases where there is a match and any unmatched\n",
    "# L cases ('both' [?] would be only cases where ther is is a match, and 'right' would\n",
    "# be cases with a match and any unmatched R columns).\n",
    "column_channel_colors = pd.merge(pd.DataFrame(pd.Series(\n",
    "    subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values)), \n",
    "                  metadata, how = 'left',\n",
    "         left_on = 0, right_on = 'full_column')[[0,'Channel']]['Channel'].map(channel_color_dict)\n",
    "\n",
    "# Set the index to be the names of the colors. There is only one column, and that is the corresponding\n",
    "# colors\n",
    "column_channel_colors.index = subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values\n",
    "\n",
    "column_channel_colors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we want to translate marker columns to their corresponding round information,\n",
    "# and then match that up with the right color, as with row columns\n",
    "\n",
    "# First, we merge the (L) non-intensity column values, transformed into a dataframe,\n",
    "# with the metadata df (R), matching on the \"0\" column present in the L,\n",
    "# which is the only column in there, with the \"full_column\" (aka df header name)\n",
    "# column in the R, only including all cases where there is a match and any unmatched\n",
    "# L cases ('both' [?] would be only cases where ther is is a match, and 'right' would\n",
    "# be cases with a match and any unmatched R columns).\n",
    "column_round_colors = pd.merge(pd.DataFrame(pd.Series(\n",
    "    subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values)), \n",
    "                  metadata, how = 'left',\n",
    "         left_on = 0, right_on = 'full_column')[[0,'Round']]['Round'].map(round_color_dict)\n",
    "\n",
    "# Set the index to be the names of the colors. There is only one column, and that is the corresponding\n",
    "# colors\n",
    "column_round_colors.index = subset_df.loc[:,~subset_df.columns.isin(not_intensities)].columns.values\n",
    "\n",
    "column_round_colors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data structure to hold everything we need for row/column annotations\n",
    "# annotations is a dictionary\n",
    "## IMPORTANT - if you use 'annotations', it MUST have both 'rows' and 'cols'\n",
    "## objects inside. These can be empty lists, but they must be there!\n",
    "anns = {}\n",
    "\n",
    "# create a data structure to hold everything we need for only row annotations\n",
    "# row_annotations is a list, where each item therein is a dictioary corresponding\n",
    "# to all of the data pertaining to that particular annotation\n",
    "# Adding each item (e.g., Sample, then Cluster), one at a time to ensure ordering\n",
    "# is as anticipated on figure\n",
    "row_annotations = []\n",
    "row_annotations.append({'label':'Sample','type':'row','mapping':sample_row_colors,'dict':sample_color_dict,\n",
    "                        'location':'center left','bbox_to_anchor':(0, 0.5)})\n",
    "row_annotations.append({'label':'Cell type','type':'row','mapping':row_celltype_colors,\n",
    "                        'dict':celltype_color_dict,\n",
    "                        'location':'lower left','bbox_to_anchor':(0, 0.65)})\n",
    "row_annotations.append({'label':'Cluster','type':'row','mapping':row_cluster_colors,\n",
    "                        'dict':cluster_color_dict,\n",
    "                        'location':'lower left','bbox_to_anchor':(0, 0.20)})\n",
    "# Add all row information into the annotations dictionary\n",
    "anns['rows'] = row_annotations\n",
    "\n",
    "\n",
    "# Now we repeat the process for column annotations\n",
    "col_annotations = []\n",
    "col_annotations.append({'label':'Round','type':'column','mapping':column_round_colors,'dict':round_color_dict,\n",
    "                       'location':'upper right','bbox_to_anchor':(1,0.50)})\n",
    "\n",
    "col_annotations.append({'label':'Column','type':'column','mapping':column_channel_colors,'dict':channel_color_dict,\n",
    "                       'location':'upper right','bbox_to_anchor':(1,0.75)})\n",
    "anns['cols'] = col_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually plot the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_function(\n",
    "    data = subset_df.loc[:,~subset_df.columns.isin(not_intensities)],\n",
    "    title = \"KMeans heatmap\",\n",
    "    # define method, metric, and color map\n",
    "    method = 'ward', metric = 'correlation', cmap = 'coolwarm',\n",
    "    # colorbar (legend coloring of main plot)   \n",
    "    cbar_kws = {'label':'Correlation'},\n",
    "    # xticklabels - want to have the nicknames instead of full names,\n",
    "    # so we translate from full to short names; we also only want to include\n",
    "    # non_intensity columns, to match the data we fed into under 'data'\n",
    "    xticklabels = [full_to_short_names[name] for name in \n",
    "                     subset_df.loc[:,\n",
    "                                 ~subset_df.columns.isin(not_intensities)].columns.values],\n",
    "    # Location where we want to save the output image\n",
    "    save_loc = output_images_dir,\n",
    "    # Boolean values for clustering\n",
    "    row_cluster = False, col_cluster = True,\n",
    "    # provide annotations established above\n",
    "    annotations = anns\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XY Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one XY map for each sample, where each cluster is a different color\n",
    "\n",
    "x_feature = 'Nuc_X'\n",
    "y_feature = 'Nuc_Y_Inv'\n",
    "\n",
    "# iterate through all samples\n",
    "for sample in ls_samples:\n",
    "    # Extract x/y coordinates and cluster ID for all cells\n",
    "    location_colors = df.loc[df['Sample_ID'] == sample,[x_feature,y_feature,'cluster']]\n",
    "\n",
    "    # Establish figure\n",
    "    fig = go.Figure()\n",
    "    title = sample\n",
    "    \n",
    "    # Iterate through all unique cluster values assocaited with this sample\n",
    "    # We do this because not every cluster may be present in every sample\n",
    "    for cluster in sorted(df.loc[df['Sample_ID'] == sample,'cluster'].unique()):\n",
    "        # Plot cells for a particular cluster ID\n",
    "        fig.add_scatter(\n",
    "            # We only want points, not points and lines, or just lines\n",
    "            mode = 'markers',\n",
    "            # Marker aesthetics\n",
    "            marker=dict(\n",
    "                size=5, opacity=0.4, # size is dot size, higher opacity = less opaque\n",
    "                color='rgb' + str(cluster_color_dict[cluster])#,\n",
    "                #line = dict(width = 2, color = 'gray') # line around each marker\n",
    "                ),\n",
    "        # X/Y data\n",
    "        x = location_colors.loc[location_colors['cluster']==cluster,x_feature],\n",
    "        y = location_colors.loc[location_colors['cluster']==cluster,y_feature],\n",
    "        name = \"Cluster \" + str(cluster))\n",
    "\n",
    "    # Update general plot aesthetics\n",
    "    fig.update_layout(title = title, plot_bgcolor = 'white', showlegend = True,\n",
    "                     legend= {'itemsizing': 'constant'}) # make the legend dots a bit bigger\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text = x_feature, linecolor = 'black')\n",
    "    fig.update_yaxes(title_text = y_feature, linecolor = 'black')\n",
    "\n",
    "    # Plot output\n",
    "    #plot(fig)\n",
    "    filename = sample + \" KMeans XY Map with \" + str(n_clusters) + \" clusters.png\"\n",
    "    filename = filename.replace(\" \", \"_\")\n",
    "    filename = os.path.join(output_images_dir, filename)\n",
    "    fig.write_image(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plotw will be counts of cells in each cluster. First, we need to create a subset of cluster info to work with. This will make things a little easier moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = pd.DataFrame(columns = ['clusternum','clustername','clustername_full','count'])\n",
    "\n",
    "# Iterate through all clusters (need 1, n.clusters+1) because we shifted all cluster IDs\n",
    "# up by one to avoid having a cluster 0\n",
    "for c in range(1,n_clusters+1):\n",
    "    cluster_counts = cluster_counts.append(pd.DataFrame(\n",
    "        {'clusternum':[c],\n",
    "         'clustername':['Cl ' + str(c)],\n",
    "         'clustername_full':['Cluster ' + str(c)],\n",
    "         'count':[df.loc[df['cluster'] == c,:].shape[0]]}))\n",
    "    \n",
    "# Set index to cluster ocunt (0-based indexing)    \n",
    "cluster_counts.index = range(cluster_counts.shape[0])\n",
    "\n",
    "cluster_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By Cluster only\n",
    "\n",
    "# Establish figure\n",
    "fig = go.Figure()\n",
    "title = 'KMeans cluster cell counts'\n",
    "\n",
    "# Plot each cluster's information separately\n",
    "for c in sorted(df.cluster.unique()):\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=cluster_counts.loc[cluster_counts['clusternum']==c,'clustername'], \n",
    "        y = cluster_counts.loc[cluster_counts['clusternum']==c,'count'],\n",
    "        text = cluster_counts.loc[cluster_counts['clusternum']==c,'count'], textposition= 'outside',\n",
    "        marker=dict(\n",
    "            color='rgb' + str(cluster_color_dict[c])),\n",
    "            showlegend = False\n",
    "    ))\n",
    "\n",
    "# Update figure aesthetics\n",
    "fig.update_layout(title = title, plot_bgcolor = 'white')\n",
    "\n",
    "# Update plots\n",
    "fig.update_xaxes(linecolor = 'black')\n",
    "fig.update_yaxes(title_text = \"Cells\", linecolor = 'black')\n",
    "\n",
    "# Figure output\n",
    "#plot(fig)\n",
    "filename = title.replace(\" \",\"_\") + \".png\"\n",
    "filename = os.path.join(output_images_dir, filename)\n",
    "fig.write_image(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plot - Proportional and count breakdowns of tissue type within each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we add columns to hold information on how many of each cell type we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type counts\n",
    "\n",
    "cluster_counts['stroma'] = cluster_counts.apply(lambda row: \n",
    "    df.loc[(df['cluster'] == row['clusternum']) &\n",
    "             (df['cell_type'] == 'STROMA'),:].shape[0] , axis = 1)\n",
    "\n",
    "cluster_counts['immune'] = cluster_counts.apply(lambda row: \n",
    "    df.loc[(df['cluster'] == row['clusternum']) &\n",
    "             (df['cell_type'] == 'IMMUNE'),:].shape[0] , axis = 1)\n",
    "\n",
    "cluster_counts['cancer'] = cluster_counts.apply(lambda row: \n",
    "    df.loc[(df['cluster'] == row['clusternum']) &\n",
    "             (df['cell_type'] == 'CANCER'),:].shape[0] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type proportions\n",
    "# Using lambdas to operate along each row to establish the value\n",
    "# in the new column for a given row\n",
    "\n",
    "cluster_counts['stroma_perc'] = cluster_counts.apply(\n",
    "    lambda row: round(row['stroma']/row['count']*100,1) , axis = 1)\n",
    "\n",
    "cluster_counts['immune_perc'] = cluster_counts.apply(\n",
    "    lambda row: round(row['immune']/row['count']*100,1) , axis = 1)\n",
    "\n",
    "cluster_counts['cancer_perc'] = cluster_counts.apply(\n",
    "    lambda row: round(row['cancer']/row['count']*100,1) , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does our dataframe look like now?\n",
    "cluster_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type within cluster - count\n",
    "\n",
    "# Establish figure\n",
    "fig = go.Figure()\n",
    "title = 'KMeans cell types count within clusters'\n",
    "\n",
    "# plot each cell type separately\n",
    "# This could also be wrapped in a for loop, as in other bar plots, if the user wishes\n",
    "# Here, the decision was made not to do so, since we want three different capitalization\n",
    "# styles and so things would be more complated than just iterating through a list of\n",
    "# ['STROMA','CANCER','IMMUNE'], for example. This was should be more readable and thus\n",
    "# easier to adapt as the user wishes.\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Stroma', x=cluster_counts['clustername'], y=cluster_counts['stroma'], \n",
    "           text = cluster_counts['stroma'], textposition='auto',\n",
    "           marker_color = 'rgb' + str(celltype_color_dict['STROMA'])),\n",
    "    \n",
    "    go.Bar(name='Immune', x=cluster_counts['clustername'], y=cluster_counts['immune'], \n",
    "           text = cluster_counts['immune'], textposition='auto',\n",
    "           marker_color = 'rgb' + str(celltype_color_dict['IMMUNE'])),\n",
    "    \n",
    "    go.Bar(name='Cancer',x=cluster_counts['clustername'], y=cluster_counts['cancer'], \n",
    "           text = cluster_counts['cancer'], textposition='auto', \n",
    "           marker_color = 'rgb' + str(celltype_color_dict['CANCER']))\n",
    "])\n",
    "\n",
    "# Update figue aesthetics\n",
    "fig.update_layout(title = title, plot_bgcolor = 'white',barmode ='stack')\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(linecolor = 'black')\n",
    "fig.update_yaxes(title = \"Cell count\", linecolor = 'black')\n",
    "\n",
    "# Plot output\n",
    "#plot(fig)\n",
    "filename = title.replace(\" \",\"_\") + \".png\"\n",
    "filename = os.path.join(output_images_dir, filename)\n",
    "fig.write_image(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell type within cluster - proportional\n",
    "\n",
    "# Establish figure\n",
    "fig = go.Figure()\n",
    "title = 'KMeans cell types proportions within clusters'\n",
    "\n",
    "\n",
    "# Plot each cell type seprately. See previous cell for an explanation.\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Stroma', x=cluster_counts['clustername'], y=cluster_counts['stroma_perc'], \n",
    "           text = cluster_counts['stroma_perc'], textposition='auto',\n",
    "           marker_color = 'rgb' + str(celltype_color_dict['STROMA'])),\n",
    "    \n",
    "    go.Bar(name='Immune', x=cluster_counts['clustername'], y=cluster_counts['immune_perc'], \n",
    "           text = cluster_counts['immune_perc'], textposition='auto',\n",
    "           marker_color = 'rgb' + str(celltype_color_dict['IMMUNE'])),\n",
    "    \n",
    "    go.Bar(name='Cancer',x=cluster_counts['clustername'], y=cluster_counts['cancer_perc'], \n",
    "           text = cluster_counts['cancer_perc'], textposition='auto', \n",
    "           marker_color = 'rgb' + str(celltype_color_dict['CANCER']))\n",
    "])\n",
    "\n",
    "# Update figure aesthetics\n",
    "fig.update_layout(title = title, plot_bgcolor = 'white',barmode ='stack')\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(linecolor = 'black')\n",
    "fig.update_yaxes(title = \"Cell proportion of total\", linecolor = 'black')\n",
    "\n",
    "# Plot output\n",
    "#plot(fig)\n",
    "filename  = title.replace(\" \",\"_\") + \".png\"\n",
    "filename = os.path.join(output_images_dir, filename)\n",
    "fig.write_image(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Boxplots distributions\n",
    "On Kmeans clusters – one plot per marker, each box is a cluster, all samples in there\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By sample ID and cell type\n",
    "\n",
    "# Define a function to be called for each marker below.\n",
    "# This function takes in a string my_marker corresponding to a colummn header\n",
    "# in the pandas dataframe df. The function then plots boxplots of the data\n",
    "# in this column, separated into different boxes by the column 'cluster',\n",
    "# which must be present in df. It returns nothing but does save the plot\n",
    "# to a png.\n",
    "def make_cluster_boxplot(my_marker, df):\n",
    "    # Establish figure\n",
    "    fig = go.Figure()\n",
    "    title = 'KMeans ' + my_marker + ' Distributions by cluster'\n",
    "\n",
    "    # Get list of clusters in order\n",
    "    clusters = sorted(df.cluster.unique())\n",
    "    data = []\n",
    "    # Append data for plotting by iterating through each cluster\n",
    "    for c in clusters:\n",
    "        data.append(go.Box(\n",
    "            # Naming choices\n",
    "            name=\"Cl \"+str(c),\n",
    "            # y-values for this cluster's data\n",
    "            y = df.loc[df['cluster']==c,my_marker],\n",
    "            # Marker aesthetics\n",
    "            marker = dict(\n",
    "                # Color for this cluster's data\n",
    "                color = 'rgb'+str(cluster_color_dict[c])\n",
    "            )\n",
    "        ))\n",
    "    \n",
    "    # Plot the data\n",
    "    fig = go.Figure(data=data)\n",
    "    \n",
    "    # Update figure aesthetics\n",
    "    fig.update_layout(title = title, plot_bgcolor = 'white',barmode ='stack', showlegend = False)\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(linecolor = 'black')\n",
    "    fig.update_yaxes(title_text = 'Z-Scored Intensity', linecolor = 'black')\n",
    "    \n",
    "    # Plot output\n",
    "    #plot(fig)    \n",
    "    filename = title.replace(\" \",\"_\") + \".png\"\n",
    "    filename = os.path.join(output_images_dir, filename)\n",
    "    fig.write_image(filename)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the plotting\n",
    "\n",
    "# Rename columns for visualization purposes\n",
    "df = df.rename(columns = full_to_short_names)\n",
    "\n",
    "# Create a plot for each marker\n",
    "# If you wanted only a subset of markers, you could say:\n",
    "# for m in ['marker1', 'marker2', 'marker3']:\n",
    "for m in [m for m in df.columns.values if m not in not_intensities]:\n",
    "    make_cluster_boxplot(m, df)\n",
    "\n",
    "# Reinstate original column names\n",
    "df = df.rename(columns = short_to_full_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the percentage/proportion of each cluster for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This information will be used in a few subsequent visualizations, as well as saved for future reference. One important thing to note about these dataframes is that Sample_ID is the name of the index, and the values of the index are our sample names. This is how the dfs are generated, and I've chosen to keep them that way because it will make it a bit easier for the median value heatmap later on. Note that the bar plot will gather its x-axis data from the index of a dataframe. Both the index (Sample_ID) and columns (cluster) are going to be named, which might make the dfs look a little unusual when printed to the screen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a dataframe of the counts for each sample/cluster combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Sample_ID, then get the value counts for each cluster,\n",
    "# then massage into a dataframe\n",
    "cluster_sample_counts = df.groupby('Sample_ID')['cluster'].value_counts().unstack().fillna(0)\n",
    "\n",
    "## Keeping some commented out code in case we want to make 'Sample_ID' a column in its own right\n",
    "#cluster_sample_counts['Sample_ID'] = cluster_sample_counts.index\n",
    "\n",
    "## Change from floats to ints\n",
    "#cluster_sample_counts.loc[:,cluster_sample_counts.columns != 'Sample_ID'] = \\\n",
    "#    cluster_sample_counts.loc[:,cluster_sample_counts.columns != 'Sample_ID'].astype(int)\n",
    "cluster_sample_counts = cluster_sample_counts.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sample_counts.head()\n",
    "\n",
    "# Note 'Sample_ID' is name of index, 'cluster' is name of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also interested in the proportion values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Proportions\n",
    "cluster_sample_props = cluster_sample_counts.copy()\n",
    "\n",
    "# Keeping some commented out code in case we've chosen\n",
    "# to make 'Sample_ID' a column in its own right\n",
    "\"\"\"cluster_sample_props.loc[:,\n",
    "    cluster_sample_props.columns != 'Sample_ID'] = \\\n",
    "        cluster_sample_props.loc[:,cluster_sample_props.columns != 'Sample_ID'].apply(\n",
    "            lambda row: round(row/row.sum()*100,1), axis =1)\"\"\"\n",
    "cluster_sample_props = \\\n",
    "        cluster_sample_props.apply(\n",
    "            lambda row: round(row/row.sum()*100,1), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sample_props\n",
    "\n",
    "# Note 'Sample_ID' is name of index, 'cluster' is name of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save files of these two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counts\n",
    "filename = \"sample_cluster_counts.csv\"\n",
    "filename = os.path.join(output_data_dir, filename)\n",
    "cluster_sample_counts.to_csv(filename, index = True, \n",
    "                             # We want the header of the output file to have the format 'cluster_#' instead of '#'\n",
    "                             ## Keeping some commented out code in case we have decided to make 'Sample_ID' a column in its own right\n",
    "                             # We also do not want to alter the name of the 'Sample_ID' column\n",
    "                             #header = [\"cluster_\" + str(c) for c in cluster_sample_counts.columns if c != 'Sample_ID'] +\\\n",
    "                             #        ['Sample_ID'])\n",
    "                            header = [\"cluster_\" + str(c) for c in cluster_sample_counts.columns])\n",
    "# Proportions\n",
    "filename = \"sample_cluster_counts_perc.csv\"\n",
    "filename = os.path.join(output_data_dir, filename)\n",
    "cluster_sample_props.to_csv(filename, index = True,\n",
    "                            # We want the header of the output file to have the format 'cluster_#' instead of '#'\n",
    "                            ## Keeping some commented out code in case we have decided to make 'Sample_ID' a column in its own right\n",
    "                             # We also do not want to alter the name of the 'Sample_ID' column\n",
    "                             #header = [\"cluster_\" + str(c) for c in cluster_sample_counts.columns if c != 'Sample_ID'] +\\\n",
    "                             #        ['Sample_ID'])\n",
    "                            header = [\"cluster_\" + str(c) for c in cluster_sample_counts.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar plot - Proportional breakdown of clusters within sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have enough information established to create this plot, we will take a detour from the cluster breakdown process to generate this plot. Then, we will continue with the above, as it is necessary for the more complicated visualizaitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By sample ID and cell type\n",
    "\n",
    "# Establish figure\n",
    "fig = go.Figure()\n",
    "title = 'Proportional breakdown of clusters within sample'\n",
    "\n",
    "# Get list of clusters\n",
    "# sorted() puts them in order\n",
    "# Remember that we need to shift the range up by 1 since the number of clusters is smaller than then name of our\n",
    "# last cluster, since we want to avoid having a Cluster 0\n",
    "clusters = sorted(list(range(1,n_clusters+1)))\n",
    "data = []\n",
    "# Iteate through each cluster and extract its data\n",
    "for c in clusters:\n",
    "    data.append(go.Bar(\n",
    "        # Display name for plt\n",
    "        name=\"Cl \"+str(c),\n",
    "        # establish x- and y-values of cluster data\n",
    "        x=cluster_sample_counts.index, # if 'Sample_ID' were a column, we would have said cluster_sample_counts['Sample_ID']\n",
    "        y=cluster_sample_counts[c],\n",
    "        # Maker aesthetics\n",
    "        marker = dict(\n",
    "            # cluster color\n",
    "            color='rgb' + str(cluster_color_dict[c])\n",
    "        )\n",
    "    ))\n",
    "\n",
    "# Plot all of the data\n",
    "fig = go.Figure(data=data)\n",
    "\n",
    "# Update figure aesthetics\n",
    "fig.update_layout(title = title, plot_bgcolor = 'white',barmode ='stack')\n",
    "\n",
    "# Update axes\n",
    "fig.update_xaxes(linecolor = 'black')\n",
    "fig.update_yaxes(title_text = \"Cluster proportion\", linecolor = 'black')\n",
    "\n",
    "# Plot output\n",
    "#plot(fig)\n",
    "filename = title.replace(\" \",\"_\") + \".png\"\n",
    "filename = os.path.join(output_images_dir, filename)\n",
    "fig.write_image(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for median value heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we will want to combine rows in `cluster_sample_counts` at will, using whichever criteria we chose. The added rows will be replaced with one sum row. Then, we will determine the proportions and only plot the samples/data we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a copy of the `cluster_sample_counts` to work with. We want to maintain the original, so that we can create as many different combinations of rows as we want without extra work to revert back to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_subset = cluster_sample_counts.copy()\n",
    "\n",
    "# We also want to rename 'Sample_ID' to 'Combination_ID'\n",
    "counts_subset.index = counts_subset.index.rename('comb_id')\n",
    "\n",
    "counts_subset.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a pandas dataframe and information on how to combin columns.\n",
    "# It sums designated columns, adds the summed row to the dataframe, drops the individual\n",
    "# rows used to create the sum, and returns the final df. The input pandas dataframe is df,\n",
    "# the rows to be dropped are identified by a list of their index identifiers, drop_rows,\n",
    "# and the new index identifier is a string called new_name.\n",
    "def row_combiner(df, drop_rows, new_name):\n",
    "    # Check that drop_rows are in the index\n",
    "    if len(set(drop_rows).intersection(set(df.index))) == 0:\n",
    "        print(\"1+ item(s) specified for dropping not found in dataframe's index.\")\n",
    "        print(\"Returning unaltered dataframe.\")\n",
    "        return df\n",
    "    \n",
    "    # First create a dataframe consisiting of the chosen summed rows\n",
    "    # Note that we are feeding a list of index identifiers into\n",
    "    # the .index.isin() portion\n",
    "    new_row = pd.DataFrame( # make a df out of the following...\n",
    "    counts_subset.loc[ # only the rows where given df has rows meeting criteria\n",
    "                    # criteria are that index identifiers are in the given list\n",
    "        df.index.isin(drop_rows),\n",
    "        # all columns are wanted, hence the \":\"\n",
    "        # We are also summming along the first (0th index) axis, along the rows\n",
    "        :].sum(axis = 0)\n",
    "            ).T # transpose to get a row instead of a column\n",
    "\n",
    "    # In order to replace the summed rows with our new row,\n",
    "    # we will need to assign our new row an index identifier\n",
    "    # This should be the identifier you want to see on the median\n",
    "    # value heatmap.\n",
    "    new_row.index = [new_name]\n",
    "\n",
    "    # We will also rename the axis to 'Sample_ID', as before,\n",
    "    # or else the counts_subset will lose its axis name when\n",
    "    # the new row is added\n",
    "    new_row = new_row.rename_axis(df.index.name, axis = 'index')\n",
    "\n",
    "    # What does this new dataframe look like?\n",
    "    # Remember 'cluster' is just the name of the columns\n",
    "    \n",
    "    df = df.append(new_row)\n",
    "    df = df.drop(drop_rows)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine specified rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_rows = the rows we are going to be replaced with the one combined row\n",
    "\n",
    "#counts_subset = \\\n",
    "#    row_combiner(df = counts_subset, drop_rows = ['patient1_data2','patient1_data2'], new_name = 'Patient_1')\n",
    "\n",
    "counts_subset = \\\n",
    "    row_combiner(df = counts_subset, drop_rows = [''], new_name = 'Patient_X')\n",
    "\n",
    "counts_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat the above process as many times as you would like to add on as many rows as you would like. There are a few other manipulations to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to drop some additional rows, wihtout replacing them with a sum? The `drop()` function is fed a list of index identifiers to drop from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts_subset.drop(['row_identifier1','row_identifier2'])\n",
    "\n",
    "counts_subset = counts_subset.drop([''])\n",
    "counts_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to rename the index name? I will not write over `cluster_sample_counts`, but here is what you would do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_subset.rename_axis('new_index_name',axis = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we made a typo, and Patient_1 should be Patient_10? Again, I am not overwriting `counts_subset` unless I set it equal to this new expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_subset.rename(index={'Patient_1':'Patient_10'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to include one or more of our dropped previously dropped rows? We can append those from the intact `counts_subset`. Here is what the resulting dataframe would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts_subset.append(cluster_sample_counts.loc[['row_identifer1'],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to create a copy of our original dataframe to use for finding the medians. As before, we will be exchanging original sample labels with other labels. Unlike before, we will not be combining rows until the very end. Row renaming will occur more simply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original df\n",
    "\n",
    "for_medians = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename rows as necessary\n",
    "\n",
    "#for_medians.loc[for_medians['Sample_ID'].isin(['patient1_data1','patient2_data2']),['Sample_ID']] = \\\n",
    "#    'Patient_1' \n",
    "\n",
    "\n",
    "for_medians.loc[for_medians['Sample_ID'].isin(['']),['Sample_ID']] = \\\n",
    "    'Patient_X' \n",
    "\n",
    "for_medians.Sample_ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a look at some rows that WERE NOT just renamed\n",
    "\n",
    "for_medians.loc[~for_medians['Sample_ID'].isin(['Patient_X']),['Sample_ID']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We ALSO need to drop rows from the `for_medians` df as necessary, so that it matches our `counts_subset` item\n",
    "# Drop rows as necessary \n",
    "\n",
    "# Drop everything that isn't 'Patient_1'\n",
    "drop_indices = for_medians.loc[for_medians['Sample_ID'] != 'Patient_X',:].index\n",
    "\n",
    "# Or maybe we want to drop everything that isn't 'Patient_1' or 'Patient_2'\n",
    "#drop_indices = for_medians.loc[~for_medians['Sample_ID'].isin(['Patient_2', 'Patient_1']),:].index\n",
    "\n",
    "# The portion through \"drop(drop_indices)\" gives us a dataframe with the rows we don't want removed\n",
    "for_medians = for_medians.drop(drop_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember `for_medians` is just our regular df with the 'Sample_ID' label changed to reflect our groupings for the visualization.\n",
    "\n",
    "#### *In order to proceed, both `counts_subset` and `for_medians` should have the same, and only the same, identifiers for sample id (data column)/comb id (index)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(for_medians.Sample_ID.unique()) == sorted(counts_subset.index.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's incorporate our cutoff. We want to avoid plotting any cluster that makes up less than *x*% of that subset of data. Now that we have counts across all samples we want to group together, we can calculate the proprotion of each cluster contribution to that grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the proportions.\n",
    "# Note that the *100 part of the command means that\n",
    "# we are dealing percentages, not proportions, so\n",
    "# that's a bit of a misnomer\n",
    "\n",
    "props_subset = \\\n",
    "        counts_subset.apply(\n",
    "            lambda row: round(row/row.sum()*100,1), axis =1)\n",
    "\n",
    "props_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff should be a percentage, not a proportion\n",
    "cutoff = 5\n",
    "\n",
    "# We will create a dataframe of boolean T/F\n",
    "# values, signifying whether the value of a\n",
    "# given grouping (row) and cluster (columns)\n",
    "# meets the inclusion criteria\n",
    "props_subset_bool = props_subset.apply(\n",
    "    lambda row: row >= cutoff, axis = 1)\n",
    "\n",
    "props_subset_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and create our `medians` dataframe now. This will be a dataframe where each row represents the medians values for markers for each sample-cluster combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#medians.columns[medians.columns.isin(not_intensities)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first group by our features of interest\n",
    "medians = for_medians.copy().groupby(['Sample_ID','cluster']).median()\n",
    "\n",
    "# then drop all columns that are not markers\n",
    "medians = medians.drop(columns = medians.columns[medians.columns.isin(not_intensities)].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize medians\n",
    "medians.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have here is a multiindex dataframe. We have two layers of row indexing, `Sample_ID` and `cluster`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `medians` and `props_subset_bool`, drop all indices in `medians` where the sample-cluster combination does not meet our proportion threshold cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_indices = []\n",
    "# iterate through first five rows in medians\n",
    "# this should be # of clusters instead!!\n",
    "for i in range(medians.shape[0]):\n",
    "#for i in range(5):\n",
    "    # extract the sample and cluster IDs from that row\n",
    "    sample = medians.iloc[i,:].name[0]\n",
    "    cluster = medians.iloc[i,:].name[1]\n",
    "    # using those sample and cluster IDs, look inside props_subset_bool\n",
    "    # look at the value specified by the correct sample and cluster col/row\n",
    "    # that T/F value therein indicates whether this cluster should be used\n",
    "    # with this sample\n",
    "    use = props_subset_bool.loc[props_subset_bool.index == sample, props_subset_bool.columns == cluster].values[0][0]\n",
    "    if not use:\n",
    "        drop_indices.append(medians.iloc[i,:].name)\n",
    "\n",
    "print(\"Dropping the following patient/cluster data from plotting due to proportion threshold failure: \" + str([i for i in drop_indices]))\n",
    "medians = medians.drop(drop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output to a dataframe\n",
    "\n",
    "filename = \"medians_patient_1_patient_2.csv\"\n",
    "filename = os.path.join(output_data_dir, filename)\n",
    "medians.to_csv(filename, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median value heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename `medians` columns for plotting\n",
    "medians = medians.rename(columns = full_to_short_names)\n",
    "medians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want the columns in a different order? Simply reorder them by asking for a df with the columns in a given order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians[['AXL','53BP1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians[['53BP1','AXL']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchor values for plot\n",
    "vmin = -2.5\n",
    "vmax = 5\n",
    "\n",
    "sb.set()\n",
    "\n",
    "# Set x- and y-axis labels\n",
    "x_axis_labels = medians.columns.values.tolist()\n",
    "y_axis_labels = [medians.iloc[i,:].name[0] + \" - Cluster \" + \n",
    "                 str(medians.iloc[i,:].name[1]) for i in range(medians.shape[0])]\n",
    "\n",
    "ax = sb.heatmap(\n",
    "    # Data for plotting by color\n",
    "    medians.loc[:,~medians.columns.isin(not_intensities)], \n",
    "    # Anchor values are those we determined above\n",
    "    vmin=vmin, vmax=vmax, \n",
    "    # Annotations are the text displayed on the plot             \n",
    "    annot=medians.loc[:,~medians.columns.isin(not_intensities)], \n",
    "    # Annotation keywords - here we determine font size            \n",
    "    annot_kws={\"size\": 4},\n",
    "    # Format of annotations - .2f means a float (decimal) value to the hundreths place\n",
    "    fmt='.2f',\n",
    "    # Add lines of specified color and length between boxes\n",
    "    linewidths = 0.33, linecolor = 'black',\n",
    "    # x- and y-axis labels are as specified above\n",
    "    xticklabels=x_axis_labels, yticklabels=y_axis_labels,\n",
    "    # Color bar keybords - provide label on color scale bar\n",
    "    cbar_kws = {'label':'Median value'},\n",
    "    # color scheme for plotting\n",
    "    cmap = 'coolwarm'\n",
    "                )\n",
    "\n",
    "# Set size of axis tick markers to 0\n",
    "ax.tick_params(length=0)\n",
    "\n",
    "# Adjust y-axis\n",
    "plt.yticks(rotation=0, size = 8)\n",
    "\n",
    "# Adjust x-axis\n",
    "ax.xaxis.tick_top() # x axis on top\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.xticks(rotation=45, size = 8)\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), ha='left') # align left\n",
    "\n",
    "# Adjust general plot aesthetics\n",
    "ax.set_title(label = \"Median values\", fontsize = 20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Plot output\n",
    "filename = \"median_values.png\"\n",
    "filename = os.path.join(output_images_dir, filename)\n",
    "plt.savefig(filename,dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any other rows or columns we want to before saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look\n",
    "df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of example, I will operate on a copy of df, called df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You MUST do df.copy()\n",
    "# 'df_copy = df' would essentially \n",
    "# give you two different names for the\n",
    "# SAME dataframe, so operating on one\n",
    "# would also operate on the other\n",
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operate on entire rows or columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "my_cols = []\n",
    "df_copy = df_copy.drop(columns = my_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only specific columns (explained below)\n",
    "my_cols = []\n",
    "my_cols = df.columns.values\n",
    "df_copy = df_copy.loc[:,my_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operate on rows and columns using filtering criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only certain rows based off of criteria\n",
    "\n",
    "# use df.loc[] to filter\n",
    "# df.loc[rows,columns]\n",
    "# df.loc[:,certain_cols] --> keep all rows ':', only certain cols\n",
    "# df.loc[certain_rows,:] --> keep only certain row, all cols ':'\n",
    "\n",
    "# Say we only want certain values for Sample_ID\n",
    "print(df_copy.Sample_ID.unique())\n",
    "keep = ['TMA1.1','TMA1.2','TMA1.3','TMA2.1','TMA2.2','TMA2.3']\n",
    "df_copy = df_copy.loc[df_copy['Sample_ID'].isin(keep),:]\n",
    "print(df_copy.Sample_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on multiple criteria\n",
    "# '&' or 'and'\n",
    "# '|' or 'or'\n",
    "# you MUST have parentheses around each logic expression!\n",
    "df_copy = df_copy.loc[\n",
    "    (df_copy['Sample_ID'].isin(['TMA1.1','TMA1.2','TMA1.3'])) \\\n",
    "    ## backslash above used to break line for readability, but tell Python to act like it's all one line\n",
    "        | (df_copy['Sample_ID'].isin(['TMA2.1','TMA2.2','TMA2.3'])),:]\n",
    "print(df_copy.Sample_ID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows based off of certain criteria\n",
    "# note the negating tilde '~'!\n",
    "\n",
    "df_copy = df_copy.loc[\n",
    "    (~df_copy['Sample_ID'].isin(['TMA1.1','TMA1.2','TMA1.3'])) \\\n",
    "    ## backslash above used to break line for readability, but tell Python to act like it's all one line\n",
    "        & (~df_copy['Sample_ID'].isin(['TMA2.1','TMA2.2','TMA2.3'])),:]\n",
    "print(df_copy.Sample_ID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data by Sample_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existence of output file first\n",
    "for sample in ls_samples:\n",
    "    filename = sample + \"_\" + step_suffix + \".csv\"\n",
    "    filename = os.path.join(output_data_dir,  filename)\n",
    "    if os.path.exists(filename):\n",
    "        print(\"File by name \"+filename+\" already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output files\n",
    "for sample in ls_samples:\n",
    "    df_save = df.loc[df['Sample_ID'] == sample,:]\n",
    "    filename = sample + \"_\" + step_suffix + \".csv\"\n",
    "    filename = os.path.join(output_data_dir, filename)\n",
    "    df_save.to_csv(filename, index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
